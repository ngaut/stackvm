{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "from sqlalchemy.orm import joinedload\n",
    "\n",
    "from app.llm.interface import LLMInterface\n",
    "from app.core.task.utils import describe_goal\n",
    "from app.config.database import SessionLocal\n",
    "from app.storage.models import Task, TaskStatus\n",
    "from notebooks.plan_mcts_optimizer import get_task_commit_tree\n",
    "\n",
    "eval_client = LLMInterface(\"openai\", \"o3-mini\")\n",
    "\n",
    "def get_answer_commits(root_commit, commit_tree) -> List:\n",
    "    if not root_commit.get(\"children\"):\n",
    "        final_answer = root_commit.get(\"vm_state\", {}).get(\"variables\", {}).get(\"final_answer\", None)\n",
    "        if final_answer:\n",
    "            return [{\n",
    "                \"commit_hash\": root_commit[\"commit_hash\"],\n",
    "                \"committed_at\": root_commit[\"committed_at\"],\n",
    "                \"description\": root_commit[\"description\"],\n",
    "                \"seq_no\": root_commit[\"seq_no\"],\n",
    "                \"commit_type\": root_commit[\"commit_type\"],\n",
    "                \"parent_hash\": root_commit[\"parent_hash\"],\n",
    "                \"final_answer\": final_answer,\n",
    "                \"reasoning\": root_commit.get(\"vm_state\", {}).get(\"reasoning\", None),\n",
    "                \"plan\": root_commit.get(\"vm_state\", {}).get(\"current_plan\", None),\n",
    "            }]\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    leaves = []\n",
    "    for child_hash in root_commit.get(\"children\"):\n",
    "        leaves.extend(get_answer_commits(commit_tree.get(child_hash), commit_tree))\n",
    "    return leaves\n",
    "\n",
    "def find_plan_commit(plan, answers):\n",
    "    for answer in answers:\n",
    "        if answer['plan'] == plan:\n",
    "            return answer\n",
    "    return None\n",
    "\n",
    "\n",
    "def list_tasks(filter_out_ids=None):\n",
    "    with SessionLocal() as session:\n",
    "        query = (\n",
    "            session.query(Task)\n",
    "            .options(joinedload(Task.namespace))\n",
    "            .filter(Task.status != TaskStatus.deleted)\n",
    "            .filter(Task.best_plan != None)\n",
    "        )\n",
    "        \n",
    "        # Add filter only if we have IDs to exclude\n",
    "        if filter_out_ids:\n",
    "            query = query.filter(Task.id.not_in(filter_out_ids))\n",
    "            \n",
    "        return query.order_by(Task.updated_at.desc()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "columns = ['id', 'goal', 'metadata', 'reasoning', 'plan', \n",
    "           'plan_commit_hash', 'answers', 'qualified_answers', \n",
    "           'is_valid', 'language']\n",
    "local_sample_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "local_sample_file = \"notebooks/local_samples.pkl\"\n",
    "try:\n",
    "    if os.path.exists(local_sample_file):\n",
    "        local_sample_df = pd.read_pickle(local_sample_file)\n",
    "    task_ids_list = local_sample_df['id'].tolist()\n",
    "    tasks = list_tasks(task_ids_list)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading local sample file: {e}\")\n",
    "    tasks = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "index = 0\n",
    "\n",
    "for task in tasks:\n",
    "    index += 1\n",
    "    print(index, \"Processing task to get plan samples\", task.id)\n",
    "    commit_tree = get_task_commit_tree(task.id)\n",
    "    if len(commit_tree) == 0:\n",
    "        print(f\"No commit tree found for task {task.id}, skipping\")\n",
    "        metadata = task.meta\n",
    "        response_format = metadata.get(\"response_format\", {})\n",
    "        sample = {\n",
    "            \"id\": task.id,\n",
    "            \"goal\": task.goal,\n",
    "            \"metadata\": metadata,\n",
    "            \"plan\": task.best_plan,\n",
    "            \"reasoning\": None,\n",
    "            \"final_answer\": None,\n",
    "            \"plan_commit_hash\": None,\n",
    "            \"answers\": [],\n",
    "            \"qualified_answers\": [],\n",
    "            \"is_valid\": True,\n",
    "            \"language\": response_format.get(\"Lang\") or response_format.get(\"lang\") if response_format else None\n",
    "        }\n",
    "        print(sample)\n",
    "        samples.append(sample)\n",
    "        continue\n",
    "    try:\n",
    "        root_commit = next(\n",
    "            commit for _, commit in commit_tree.items() if not commit[\"parent_hash\"]\n",
    "        )\n",
    "    except StopIteration:\n",
    "        print(f\"No root commit found for task {task.id}\")\n",
    "        continue\n",
    "\n",
    "    leaves = get_answer_commits(root_commit, commit_tree)\n",
    "    try:\n",
    "        metadata = task.meta\n",
    "        answer_commit = find_plan_commit(task.best_plan, leaves)\n",
    "        response_format = metadata.get(\"response_format\", {})\n",
    "        sample = {\n",
    "            \"id\": task.id,\n",
    "            \"goal\": task.goal,\n",
    "            \"metadata\": metadata,\n",
    "            \"plan\": task.best_plan,\n",
    "            \"reasoning\": answer_commit.get(\"reasoning\") if answer_commit else None,\n",
    "            \"final_answer\": answer_commit.get(\"final_answer\") if answer_commit else None,\n",
    "            \"plan_commit_hash\": answer_commit.get(\"commit_hash\") if answer_commit else None,\n",
    "            \"answers\": leaves,\n",
    "            \"qualified_answers\": [],\n",
    "            \"is_valid\": True,\n",
    "            \"language\": response_format.get(\"Lang\") or response_format.get(\"lang\") if response_format else None\n",
    "        }\n",
    "        print(sample)\n",
    "        samples.append(sample)\n",
    "    except Exception as e:\n",
    "        print(f\"Error describing goal for task {task.id}: {e}. Data {task.meta}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_samples_df = pd.DataFrame(samples)\n",
    "local_sample_df = pd.concat([local_sample_df, new_samples_df], ignore_index=True)\n",
    "\n",
    "# Save updated DataFrame back to file\n",
    "local_sample_df.to_pickle(local_sample_file)\n",
    "\n",
    "print(f\"Added {len(samples)} new samples. Total samples: {len(local_sample_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.json import extract_json\n",
    "\n",
    "def _evaluate_criteria(llm_client, prompt_template: str, context: dict) -> tuple[int, str]:\n",
    "    \"\"\"Evaluate a single quality criteria using LLM.\n",
    "    \n",
    "    Args:\n",
    "        llm_client: Initialized LLM client\n",
    "        prompt_template: Prompt template with placeholders\n",
    "        context: Dictionary containing template variables\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (score, explanation) with fail-safes for errors\n",
    "    \"\"\"\n",
    "    try:\n",
    "        formatted_prompt = prompt_template.format(**context)\n",
    "        response = llm_client.generate(formatted_prompt)\n",
    "        json_obj = extract_json(response)\n",
    "        return json_obj.get(\"score\", 0), json_obj.get(\"explanation\", \"No explanation provided\")\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {str(e)}, data: {response}\")\n",
    "        raise e\n",
    "\n",
    "def evaluate_and_filter_answers(llm_client, row, threshold: int = 7) -> list[dict]:\n",
    "    \"\"\"Evaluate answers against quality criteria and filter based on scores.\n",
    "    \n",
    "    Args:\n",
    "        llm_client: Initialized LLM client interface\n",
    "        row: DataFrame row containing goal and answers\n",
    "        threshold: Minimum score required for retention (0-10 scale)\n",
    "        \n",
    "    Returns:\n",
    "        List of filtered answer dictionaries with evaluation metadata\n",
    "    \"\"\"\n",
    "    filtered_answers = []\n",
    "    goal_desc = describe_goal(row['goal'], row['metadata'])\n",
    "    \n",
    "    for answer in row['answers']:\n",
    "        print(f\"Evaluating answer: {answer['commit_hash']}\")\n",
    "        # Prepare evaluation context\n",
    "        context = {\n",
    "            'goal_desc': goal_desc,\n",
    "            'final_answer': answer.get('final_answer', ''),\n",
    "            'plan': str(answer.get('plan', '')),\n",
    "            'response_lang': row['metadata'].get('response_language', 'match goal language')\n",
    "        }\n",
    "\n",
    "        # 1. Language Consistency Evaluation\n",
    "        lang_prompt = \"\"\"Evaluate language consistency between goal and answer:\n",
    "Goal: {goal_desc} (Response Language: {response_lang})\n",
    "Answer: {final_answer}\n",
    "\n",
    "Checklist:\n",
    "✓ Language matches specified response language\n",
    "✓ Consistent terminology usage\n",
    "✓ No unintentional code-switching\n",
    "✓ Grammar/syntax correctness\n",
    "\n",
    "Provide your evaluation in this JSON format:\n",
    "\n",
    "{{\n",
    "    \"score\": [0-10], # 10 is the highest score means the plan and answer are perfect.\n",
    "    \"explanation\": \"Detailed explanation of the score...\"\n",
    "}}\"\"\"\n",
    "\n",
    "        lang_score, lang_expl = _evaluate_criteria(llm_client, lang_prompt, context)\n",
    "\n",
    "        # 2. Logical Flow Evaluation  \n",
    "        logic_prompt = \"\"\"Analyze logical consistency of this plan for '{goal_desc}':\n",
    "Plan: {plan}\n",
    "\n",
    "Checklist:\n",
    "✓ Step-by-step progression without gaps\n",
    "✓ No contradictory statements\n",
    "✓ Clear termination condition\n",
    "✓ Appropriate error handling\n",
    "✓ Unnecessary repeated steps (Note: Different tools for the same query are intentional design choices and should NOT be considered redundant.)\n",
    "✓ Redundant variables\n",
    "✓ Over-engineering indicators\n",
    "\n",
    "Note: Different query methods for the same target are intentional design choices and should NOT be considered redundant.\n",
    "\n",
    "\n",
    "Provide your evaluation in this JSON format:\n",
    "\n",
    "{{\n",
    "    \"score\": [0-10], # 10 is the highest score means the plan is perfect.\n",
    "    \"explanation\": \"Detailed explanation of the score...\"\n",
    "}}\"\"\"\n",
    "        logic_score, logic_expl = _evaluate_criteria(llm_client, logic_prompt, context)\n",
    "\n",
    "        # Store evaluation metadata\n",
    "        evaluation_res = {\n",
    "            'lang_score': lang_score,\n",
    "            'lang_explanation': lang_expl,\n",
    "            'logic_score': logic_score,\n",
    "            'logic_explanation': logic_expl,\n",
    "        }\n",
    "        answer.update(evaluation_res)\n",
    "        if all(score >= threshold for score in [lang_score, logic_score]):\n",
    "            filtered_answers.append(answer)\n",
    "        else:\n",
    "            print(f\"Answer {answer['commit_hash']} failed one or more checks:{evaluation_res}\")\n",
    "    \n",
    "    return filtered_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Evaluate unevaluated samples\n",
    "for index, row in tqdm(local_sample_df.iterrows(), total=len(local_sample_df), desc=\"Evaluating answers\"):\n",
    "    # Skip if already evaluated (qualified_answers is not empty)\n",
    "    if row['qualified_answers'] and len(row['qualified_answers']) > 0:\n",
    "        continue\n",
    "\n",
    "    if row['answers'] is None or len(row['answers']) == 0:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Evaluate and update qualified_answers\n",
    "        qualified_answers = evaluate_and_filter_answers(eval_client, row)\n",
    "        local_sample_df.at[index, 'qualified_answers'] = qualified_answers\n",
    "        local_sample_df.at[index, 'is_valid'] = len(qualified_answers) > 0 and row['plan_commit_hash'] in [ans['commit_hash'] for ans in qualified_answers]\n",
    "        \n",
    "        # Save periodically (e.g., every 10 samples)\n",
    "        if index % 10 == 0:\n",
    "            local_sample_df.to_pickle(local_sample_file)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {index}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Save final results\n",
    "local_sample_df.to_pickle(local_sample_file)\n",
    "\n",
    "# Print summary\n",
    "total_evaluated = len(local_sample_df[local_sample_df['is_valid'] == True])\n",
    "print(f\"Evaluation complete. {total_evaluated} samples are valid (have qualified answers and valid best plan).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_plan(llm_client, row) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate and select the best plan from filtered answers using LLM.\n",
    "    Returns the commit hash of the best plan.\n",
    "    \"\"\"\n",
    "    if not row['filtered_answers']:\n",
    "        return None\n",
    "    \n",
    "    # Prepare evaluation prompt\n",
    "    prompt = f\"\"\"Evaluate these plan candidates for the goal: {row['goal_description']}\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. Answer Quality: Correctness, completeness, and clarity of final answer\n",
    "2. Plan Logic: Logical flow, step progression, and error handling\n",
    "3. Efficiency: Minimal redundant steps while maintaining intentional design choices\n",
    "4. Documentation: Clear reasoning and plan documentation\n",
    "\n",
    "Plans to Evaluate:\n",
    "\"\"\"\n",
    "    for idx, answer in enumerate(row['filtered_answers'], 1):\n",
    "        prompt += f\"\\nPlan {idx} (Commit: {answer['commit_hash']}):\\n\"\n",
    "        prompt += f\"- Final Answer: {answer['final_answer']}\\n\"\n",
    "        prompt += f\"- Plan: {answer['plan']}\\n\"\n",
    "        prompt += f\"- Reasoning: {answer['reasoning']}\\n\"\n",
    "\n",
    "    prompt += \"\"\"\n",
    "Output MUST be JSON format:\n",
    "{\n",
    "    \"best_commit\": \"commit_hash\",\n",
    "    \"explanation\": \"Detailed comparison analysis...\"\n",
    "}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm_client.generate(prompt)\n",
    "        result = extract_json(response)\n",
    "        return result.get('best_commit')\n",
    "    except Exception as e:\n",
    "        print(f\"Best plan evaluation failed for task {row['task_id']}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_reasoning(llm_client, row):\n",
    "    goal = row['goal']\n",
    "    best_plan = row['plan']\n",
    "    final_answer = None\n",
    "    for answer in row['answers']:\n",
    "        if row['plan_commit_hash'] == answer['commit_hash']:\n",
    "            final_answer = answer['final_answer']\n",
    "            break\n",
    "    \n",
    "    prompt = (\n",
    "        'You are tasked with generating a reasoning process based on a given goal, which should lead to a specific plan and final answer. However, the reasoning must represent the detailed thought process that occurs *before* the plan is created and the final answer is determined. Here\\'s the information:\\n\\n'\n",
    "        f'Goal: \"{goal}\"\\n'\n",
    "        f'Reference Plan (do not mention in output): \"{best_plan}\"\\n'\n",
    "        f'Reference Final Answer (do not mention in output): \"{final_answer}\"\\n\\n'\n",
    "        'Your task is to write a reasoning narrative that:\\n'\n",
    "        '- Starts with the goal and explores how to approach it.\\n'\n",
    "        '- Includes logical, detailed steps of thinking, such as identifying what needs to be done, breaking it into smaller sub-goals, considering possible methods or tools, reflecting on potential challenges or alternatives, and systematically narrowing down the approach.\\n'\n",
    "        '- Uses a natural, human-like thought process (e.g., \"I need to... First, I\\'ll... Then considering... What if... Maybe I should also check... This makes sense because... Therefore...\").\\n'\n",
    "        '- Does NOT explicitly mention the specific steps of the reference plan or the reference final answer, but ensures the reasoning logically and naturally builds toward them.\\n'\n",
    "        '- Remains a general thought process that could precede the creation of the plan and answer.\\n'\n",
    "        '- Must be detailed and thorough, providing a rich exploration of the problem with high-quality insights (e.g., justifying choices, weighing pros and cons, considering edge cases), while avoiding irrelevant tangents or redundant filler.\\n'\n",
    "        '- Should feel like a coherent, step-by-step intellectual journey that demonstrates deep engagement with the goal.\\n\\n'\n",
    "        'Output the reasoning as a concise yet detailed paragraph or a short sequence of thoughts. Here\\'s an example for reference:\\n\\n'\n",
    "        'Example Goal: \"Find the value of x in the equation x + 3 = 7\"\\n'\n",
    "        'Example Reference Plan: \"Step 1: Subtract 3 from both sides. Step 2: Simplify to find x.\"\\n'\n",
    "        'Example Reference Final Answer: \"4\"\\n'\n",
    "        'Example Reasoning: \"I need to figure out what x equals in this equation. Looking at it, I see x is being added to 3, and that total is 7. My goal is to isolate x, so I need to think about how to get rid of the 3. What’s happening here mathematically? Addition is tying x and 3 together, so maybe I can reverse that operation. If I subtract something, though, won’t that change the equation? Wait—I remember equations need balance, so if I adjust one side, I have to adjust the other too. Let’s explore: if I take 3 away from 7, I’d get a number, but I need x alone on the other side. So, subtracting 3 from both sides might work. Could there be another way, like multiplying or dividing? No, that doesn’t fit here—addition’s the key. This feels promising; I’ll proceed by simplifying after balancing both sides, and that should reveal x.\"\\n\\n'\n",
    "        f'Now, generate the reasoning for the given goal: \"{goal}\", ensuring it aligns with the reference plan \"{best_plan}\" and reference final answer \"{final_answer}\" without mentioning their specific details. Provide a detailed, high-quality thought process that deeply engages with the goal.'\n",
    "    )\n",
    "\n",
    "    if final_answer is None:\n",
    "        prompt = (\n",
    "            'You are tasked with generating a reasoning process based on a given goal, which should lead to a specific plan. However, the reasoning must represent the detailed thought process that occurs *before* the plan is created. Here\\'s the information:\\n\\n'\n",
    "            f'Goal: \"{goal}\"\\n'\n",
    "            f'Reference Plan (do not mention in output): \"{best_plan}\"\\n'\n",
    "            'Your task is to write a reasoning narrative that:\\n'\n",
    "            '- Starts with the goal and explores how to approach it.\\n'\n",
    "            '- Includes logical, detailed steps of thinking, such as identifying what needs to be done, breaking it into smaller sub-goals, considering possible methods or tools, reflecting on potential challenges or alternatives, and systematically narrowing down the approach.\\n'\n",
    "            '- Uses a natural, human-like thought process (e.g., \"I need to... First, I\\'ll... Then considering... What if... Maybe I should also check... This makes sense because... Therefore...\").\\n'\n",
    "            '- Does NOT explicitly mention the specific steps of the reference plan, but ensures the reasoning logically and naturally builds toward them.\\n'\n",
    "            '- Remains a general thought process that could precede the creation of the plan and answer.\\n'\n",
    "            '- Must be detailed and thorough, providing a rich exploration of the problem with high-quality insights (e.g., justifying choices, weighing pros and cons, considering edge cases), while avoiding irrelevant tangents or redundant filler.\\n'\n",
    "            '- Should feel like a coherent, step-by-step intellectual journey that demonstrates deep engagement with the goal.\\n\\n'\n",
    "            'Output the reasoning as a concise yet detailed paragraph or a short sequence of thoughts. Here\\'s an example for reference:\\n\\n'\n",
    "            'Example Goal: \"Find the value of x in the equation x + 3 = 7\"\\n'\n",
    "            'Example Reference Plan: \"Step 1: Subtract 3 from both sides. Step 2: Simplify to find x.\"\\n'\n",
    "            'Example Reasoning: \"I need to figure out what x equals in this equation. Looking at it, I see x is being added to 3, and that total is 7. My goal is to isolate x, so I need to think about how to get rid of the 3. What’s happening here mathematically? Addition is tying x and 3 together, so maybe I can reverse that operation. If I subtract something, though, won’t that change the equation? Wait—I remember equations need balance, so if I adjust one side, I have to adjust the other too. Let’s explore: if I take 3 away from 7, I’d get a number, but I need x alone on the other side. So, subtracting 3 from both sides might work. Could there be another way, like multiplying or dividing? No, that doesn’t fit here—addition’s the key. This feels promising; I’ll proceed by simplifying after balancing both sides, and that should reveal x.\"\\n\\n'\n",
    "            f'Now, generate the reasoning for the given goal: \"{goal}\", ensuring it aligns with the reference plan \"{best_plan}\"\" without mentioning their specific details. Provide a detailed, high-quality thought process that deeply engages with the goal.'\n",
    "        )\n",
    "    \n",
    "    reasoning = llm_client.generate(prompt)\n",
    "    return reasoning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Process each row to generate missing reasoning\n",
    "for index, row in tqdm(local_sample_df.iterrows(), total=len(local_sample_df), desc=\"Generating reasoning\"):\n",
    "    # Skip if reasoning already exists and is not empty\n",
    "    if row['reasoning'] and isinstance(row['reasoning'], str) and len(row['reasoning'].strip()) > 0:\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # Generate reasoning for this row\n",
    "        reasoning = generate_reasoning(eval_client, row)\n",
    "        local_sample_df.at[index, 'reasoning'] = reasoning\n",
    "        \n",
    "        # Save periodically to prevent data loss\n",
    "        if index % 10 == 0:\n",
    "            local_sample_df.to_pickle(local_sample_file)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating reasoning for row {index}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Save final results\n",
    "local_sample_df.to_pickle(local_sample_file)\n",
    "\n",
    "# Print summary\n",
    "total_with_reasoning = len(local_sample_df[local_sample_df['reasoning'].notna()])\n",
    "print(f\"Processing complete. {total_with_reasoning} samples now have reasoning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_plan_steps(plan):\n",
    "    \"\"\"\n",
    "    remove useless steps\n",
    "    \"\"\"\n",
    "    if not plan or not isinstance(plan, list):\n",
    "        return plan\n",
    "    \n",
    "    origin_length = len(plan)\n",
    "    \n",
    "    # deep copy to avoid modifying original data\n",
    "    new_plan = [step.copy() for step in plan if isinstance(step, dict)]\n",
    "    \n",
    "    # check the first step\n",
    "    if new_plan and new_plan[0].get('type') == 'reasoning' and new_plan[0].get('seq_no') == 0:\n",
    "        # delete the first step\n",
    "        new_plan.pop(0)\n",
    "        \n",
    "        # re-number the remaining steps\n",
    "        for idx, step in enumerate(new_plan):\n",
    "            step['seq_no'] = idx\n",
    "\n",
    "        if len(new_plan) != origin_length - 1:\n",
    "            raise ValueError(f\"The number of steps in the adjusted plan does not match the original plan: {len(new_plan)} != {origin_length - 1}\")\n",
    "        \n",
    "        expected_seq = 0\n",
    "        for step in new_plan:\n",
    "            if step.get('seq_no') != expected_seq:\n",
    "                raise ValueError(f\"The sequence of steps in the adjusted plan is not continuous: {step.get('seq_no')} != {expected_seq}\")\n",
    "            expected_seq += 1\n",
    "        return new_plan\n",
    "    else:\n",
    "        if len(new_plan) != origin_length:\n",
    "            raise ValueError(f\"The number of steps in the adjusted plan does not match the original plan: {len(new_plan)} != {origin_length}\")\n",
    "    \n",
    "    return new_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ianz/miniconda3/envs/stackvm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Adjusting plans: 100%|██████████| 175/175 [00:00<00:00, 14507.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Process each row to adjust plans\n",
    "for index, row in tqdm(local_sample_df.iterrows(), total=len(local_sample_df), desc=\"Adjusting plans\"):\n",
    "    try:\n",
    "        # Adjust plan for this row\n",
    "        new_plan = adjust_plan_steps(row['plan'])\n",
    "        local_sample_df.at[index, 'plan'] = new_plan\n",
    "    except Exception as e:\n",
    "        print(f\"Error adjusting plan for row {index}: {str(e)}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['id','goal', 'metadata', 'reasoning', 'plan']\n",
    "dataset_df =   local_sample_df[selected_columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'goal', 'metadata', 'reasoning', 'plan'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'goal', 'metadata', 'reasoning', 'plan'],\n",
       "    num_rows: 175\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "for col in [\"metadata\", \"plan\"]:\n",
    "    dataset_df[col] = dataset_df[col].apply(lambda x: json.dumps(x) if isinstance(x, (dict, list)) else x)\n",
    "\n",
    "dataset_df = dataset_df.reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(dataset_df)\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 212.27ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.98s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/ianthereal-z/tidb_bot/commit/3796106600b99d00bec693b4d881fedd3093d9d7', commit_message='Upload dataset', commit_description='', oid='3796106600b99d00bec693b4d881fedd3093d9d7', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/ianthereal-z/tidb_bot', endpoint='https://huggingface.co', repo_type='dataset', repo_id='ianthereal-z/tidb_bot'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.push_to_hub(\"ianthereal-z/tidb_bot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

OPENAI_API_KEY=your_openai_api_key
AUTOFLOW_API_KEY=your_autoflow_api_key
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o
# Reasoning model configuration (optional)
REASON_LLM_PROVIDER=openai
REASON_LLM_MODEL=deepseek/deepseek-r1

# Evaluation model configuration (optional)
EVALUATION_LLM_PROVIDER=gemini
EVALUATION_LLM_MODEL=gemini-2.0-flash

AWS_DEFAULT_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

AUTOFLOW_BASE_URL=https://tidb.ai
KB_ID=30001
DATABASE_URI=mysql+pymysql://your_username:password@host:port/stackvm?ssl_ca=/etc/ssl/cert.pem&ssl_verify_cert=true&ssl_verify_identity=true
OLLAMA_DEBUG=1
OLLAMA_BASE_URL=http://localhost:11434
OPENAI_BASE_URL=https://api.openai.com/v1/

# stackvm frontend url
BACKEND_CORS_ORIGINS = ["http://localhost:5173"]

# Optional: Model specific configurations
# Override default configurations for specific models
# Example:
MODEL_CONFIGS='{"gpt-4o":{"temperature":0},"o3-mini":{"reasoning_effort":"high"}}'

# If MODEL_CONFIGS is not set, default configurations will be used:
# gpt-4o: {"temperature": 0}
# o3-mini: { "reasoning_effort": "high"}
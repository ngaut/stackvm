{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add the project root to Python path\n",
    "\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "from notebooks.optimize_plan import get_task_answer, update_plan, execute_task_using_new_plan, evaulate_task_answer, stackvm_host\n",
    "\n",
    "def get_evaluation_pending_tasks(        \n",
    "    start_time: Optional[datetime] = None,\n",
    "    end_time: Optional[datetime] = None,\n",
    "    evaluation_statuses: Optional[List[str]] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches the list of tasks pending evaluation from the API.\n",
    "\n",
    "    Args:\n",
    "        start_time (Optional[datetime]): The start time to filter tasks.\n",
    "        end_time (Optional[datetime]): The end time to filter tasks.\n",
    "        evaluation_statuses (Optional[List[str]]): List of evaluation statuses to filter by. Defaults to ['NOT_EVALUATED'].\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of tasks pending evaluation.\n",
    "    \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the request fails.\n",
    "        ValueError: If the response cannot be decoded.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{stackvm_host}/api/tasks/evaluation\"\n",
    "    params = {}\n",
    "    \n",
    "    if start_time:\n",
    "        params['start_time'] = start_time.isoformat()\n",
    "    if end_time:\n",
    "        params['end_time'] = end_time.isoformat()\n",
    "    if evaluation_statuses:\n",
    "        # Join multiple statuses with commas\n",
    "        params['evaluation_status'] = ','.join(evaluation_statuses)\n",
    "    else:\n",
    "        # Default to NOT_EVALUATED if no statuses are provided\n",
    "        params['evaluation_status'] = 'NOT_EVALUATED'\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4XX or 5XX)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Unexpected response format: Expected a list of tasks.\")\n",
    "        \n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle network-related errors\n",
    "        print(f\"An error occurred while making the request: {e}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        # Handle JSON decoding errors or unexpected data formats\n",
    "        print(f\"An error occurred while processing the response: {ve}\")\n",
    "        raise\n",
    "\n",
    "def record_evaluation(\n",
    "    task_id: str,\n",
    "    evaluation_status: str,\n",
    "    evaluation_reason: Optional[str] = \"\",\n",
    "    timeout: int = 60\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Records the evaluation result of a specific task by calling the API endpoint.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API (e.g., 'http://stackvm-dev.tidb.ai:5556').\n",
    "        task_id (str): The ID of the task to be evaluated.\n",
    "        evaluation_status (str): The evaluation status (e.g., \"APPROVED\", \"REJECTED\").\n",
    "        evaluation_reason (Optional[str]): The reason for the evaluation decision.\n",
    "        api_token (Optional[str]): API token for authentication, if required.\n",
    "        timeout (int): Timeout in seconds for the API request.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The JSON response from the API indicating success or failure.\n",
    "    \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the request fails.\n",
    "        ValueError: If the response cannot be decoded or contains an error.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{stackvm_host}/api/tasks/{task_id}/evaluation\"\n",
    "    payload = {\n",
    "        \"evaluation_status\": evaluation_status,\n",
    "        \"evaluation_reason\": evaluation_reason\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(endpoint, json=payload, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"Unexpected response format: Expected a JSON object.\")\n",
    "\n",
    "        if not data.get(\"success\", False):\n",
    "            error_message = data.get(\"error\", \"Unknown error occurred.\")\n",
    "            raise ValueError(f\"API Error: {error_message}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while making the request: {e}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"An error occurred while processing the response: {ve}\")\n",
    "        raise\n",
    "\n",
    "def record_human_evaluation(\n",
    "    task_id: str,\n",
    "    evaluation_status: str,\n",
    "    feedback: Optional[str] = \"\",\n",
    "    timeout: int = 60\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Records the evaluation result of a specific task by calling the API endpoint.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API (e.g., 'http://stackvm-dev.tidb.ai:5556').\n",
    "        task_id (str): The ID of the task to be evaluated.\n",
    "        evaluation_status (str): The evaluation status (e.g., \"APPROVED\", \"REJECTED\").\n",
    "        evaluation_reason (Optional[str]): The reason for the evaluation decision.\n",
    "        api_token (Optional[str]): API token for authentication, if required.\n",
    "        timeout (int): Timeout in seconds for the API request.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The JSON response from the API indicating success or failure.\n",
    "    \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the request fails.\n",
    "        ValueError: If the response cannot be decoded or contains an error.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{stackvm_host}/api/tasks/{task_id}/human_evaluation\"\n",
    "    payload = {\n",
    "        \"evaluation_status\": evaluation_status,\n",
    "        \"feedback\": feedback\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(endpoint, json=payload, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"Unexpected response format: Expected a JSON object.\")\n",
    "\n",
    "        if not data.get(\"success\", False):\n",
    "            error_message = data.get(\"error\", \"Unknown error occurred.\")\n",
    "            raise ValueError(f\"API Error: {error_message}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while making the request: {e}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"An error occurred while processing the response: {ve}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from app.utils.json import extract_json\n",
    "\n",
    "def optimize_plan(task_id:str, branch_name:Optional[str]=\"main\", max_iteration=2):\n",
    "    current_branch_name = branch_name\n",
    "    error_message = None\n",
    "    iteration_round = 0\n",
    "\n",
    "    while True:\n",
    "        print(f\"Start to evaluate plan for task(id={task_id},branch={current_branch_name})\")\n",
    "        detail = get_task_answer(task_id, current_branch_name)\n",
    "\n",
    "        if detail is not None:\n",
    "            goal = detail.get(\"goal\")\n",
    "            final_answer = detail.get(\"final_answer\")\n",
    "            plan = detail.get(\"plan\")\n",
    "            metadata = detail.get(\"metadata\")\n",
    "\n",
    "            response = evaulate_task_answer(goal, metadata, final_answer, plan)\n",
    "            try:\n",
    "                eval_res_str = extract_json(response)\n",
    "                eval_res = json.loads(eval_res_str)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to decode evaluation result {e}: {response}\")\n",
    "                return\n",
    "\n",
    "            eval_status = \"APPROVED\" if eval_res.get(\"accept\", False) else \"REJECTED\"\n",
    "            eval_reason = json.dumps(eval_res, indent=2) \n",
    "\n",
    "            record_evaluation(task_id, eval_status, eval_reason)\n",
    "\n",
    "            if eval_res.get(\"accept\", False) is True:\n",
    "                print(f\"Goal Pass! {goal}, evaluation result:{eval_reason}\")\n",
    "                return\n",
    "\n",
    "            print(f\"Goal Not Pass! {goal}, the evaluation result:{eval_reason}\")\n",
    "\n",
    "            if iteration_round >= max_iteration:\n",
    "                break\n",
    "\n",
    "            revised_plan_response = update_plan(goal, metadata, plan, eval_reason)\n",
    "\n",
    "            try:\n",
    "                revised_plan_str = extract_json(revised_plan_response)\n",
    "                revised_plan = json.loads(revised_plan_str)\n",
    "            except Exception as e:\n",
    "                error_message = f\"Failed to decode revised plan {e}: {revised_plan_response}\"\n",
    "                break\n",
    "\n",
    "            print(\"revised plan:\", revised_plan)\n",
    "\n",
    "            try:\n",
    "                updated_result = execute_task_using_new_plan(task_id, revised_plan)\n",
    "                print(f\"Revised plan execution result {updated_result}\")\n",
    "            except Exception as e:\n",
    "                error_message = f\"Failed to execute task using new plan {e}\"\n",
    "                break\n",
    "            \n",
    "            current_branch_name = updated_result.get(\"branch_name\", None)\n",
    "            current_final_answer = updated_result.get(\"final_answer\", None)\n",
    "            if current_branch_name is None or current_final_answer is None:\n",
    "                error_message = \"Failed to execut task using new plan, get empty answer\"\n",
    "                break\n",
    "\n",
    "            iteration_round += 1\n",
    "    \n",
    "    if error_message is None:\n",
    "        error_message = \"Still failed after two evaluations round.\"\n",
    "    record_human_evaluation(task_id, \"WAITTING_FOR_EVALUATION\", error_message)\n",
    "            \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to evaluate plan for task(id=858e373f-09df-4e1c-8b64-e7413c4aeb2d,branch=main)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:25:01,591 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal:How can we set up a replication strategy in TiDB for backing up over 50TB of data from a TiDB-v4.0.14 cluster and restoring it to a new cluster as part of a disaster recovery strategy, using BR and TiCDC, without impacting production performance or compromising storage utilization?, the evaluation result:{'accept': False, 'answer_quality_assessment_explanation': 'The final answer does not fully resolve the goal as it lacks specific guidance on how to manage garbage collection settings, which is a critical aspect mentioned in the supplementary information. The user seeks an alternative approach to manage large-scale data backup and restore in TiDB without disabling garbage collection or significantly increasing its duration. The answer provides a comprehensive plan for backup and replication but does not address the garbage collection issue, which could impact production performance and storage efficiency. This omission fails to meet the guideline of providing a direct problem resolution.', 'plan_adjustment_suggestion': \"To improve the plan, it should include a section specifically addressing garbage collection settings in TiDB. This could involve strategies for tuning garbage collection parameters to minimize its impact during backup and replication processes. Additionally, the plan should explore alternative methods or tools that can handle garbage collection more efficiently without compromising performance. The plan should also include a step to evaluate the current garbage collection settings and provide recommendations for adjustments based on the cluster's workload and data size. This would ensure the plan aligns with the user's goal of maintaining production performance and storage efficiency.\", 'goal_classification': 'Direct Problem Resolution'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:25:37,328 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revised plan: [{'seq_no': 0, 'type': 'reasoning', 'parameters': {'chain_of_thoughts': 'To set up a replication strategy in TiDB for backing up over 50TB of data from a TiDB-v4.0.14 cluster and restoring it to a new cluster as part of a disaster recovery strategy, we will use BR (Backup & Restore) for full data backup and TiCDC for incremental data replication. The plan will ensure minimal impact on production performance and efficient storage utilization.\\n\\n1. **Overall Strategy**:\\n   - Use BR for full backup of the existing cluster.\\n   - Use TiCDC to capture and replicate incremental changes.\\n   - Restore the full backup to the new cluster.\\n   - Apply incremental changes to the new cluster.\\n\\n2. **Key Decision Points**:\\n   - Scheduling backups during low-traffic periods to minimize performance impact.\\n   - Configuring TiCDC to efficiently handle incremental data without overwhelming the network or storage.\\n\\n3. **Assumptions**:\\n   - The production environment can tolerate scheduled backup windows.\\n   - Network bandwidth is sufficient for TiCDC replication.\\n\\n4. **Compliance Checks**:\\n   - ✓ No user-specific queries planned (will not attempt to detect current version/configuration)\\n   - ✓ All responses will maintain consistent language (English)\\n   - ✓ Final recommendations will be stored in final_answer\\n   - ✓ All variable references use correct ${var} syntax\\n\\n5. **Expected Outcomes**:\\n   - A reliable backup and restore strategy that ensures data integrity and availability.\\n   - Minimal impact on production performance during backup and replication.\\n\\n6. **Limitations**:\\n   - The initial full backup may require significant time and resources.\\n   - Network issues could affect TiCDC replication efficiency.', 'dependency_analysis': 'Step 2 depends on Step 1. Step 3 depends on Step 2. Step 4 depends on Step 3.'}}, {'seq_no': 1, 'type': 'assign', 'parameters': {'backup_schedule': 'off-peak hours', 'br_options': '--ratelimit 100MB', 'ticdc_options': '--sink-uri=\"mysql://user:password@target-cluster:3306/\"'}}, {'seq_no': 2, 'type': 'calling', 'parameters': {'output_vars': ['br_backup_plan'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'Backup schedule: ${backup_schedule}, BR options: ${br_options}', 'prompt': 'Generate a detailed plan for using BR to perform a full backup of a TiDB-v4.0.14 cluster with over 50TB of data. Include considerations for minimizing impact on production performance and storage utilization. Please ensure that the generated text uses English.'}}}, {'seq_no': 3, 'type': 'calling', 'parameters': {'output_vars': ['ticdc_replication_plan'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'TiCDC options: ${ticdc_options}', 'prompt': 'Generate a detailed plan for using TiCDC to replicate incremental changes from a TiDB-v4.0.14 cluster to a new cluster. Include considerations for minimizing network and storage impact. Please ensure that the generated text uses English.'}}}, {'seq_no': 4, 'type': 'calling', 'parameters': {'output_vars': ['gc_tuning_plan'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'TiDB garbage collection settings and their impact on performance and storage', 'prompt': 'Provide a detailed plan for tuning garbage collection settings in TiDB to minimize impact during backup and replication processes. Include strategies for adjusting parameters based on workload and data size. Please ensure that the generated text uses English.'}}}, {'seq_no': 5, 'type': 'calling', 'parameters': {'output_vars': ['restore_process'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'BR backup plan: ${br_backup_plan}, TiCDC replication plan: ${ticdc_replication_plan}, GC tuning plan: ${gc_tuning_plan}', 'prompt': 'Describe the process of restoring a full backup to a new TiDB cluster and applying incremental changes using TiCDC. Ensure the process maintains data integrity and availability. Please ensure that the generated text uses English.'}}}, {'seq_no': 6, 'type': 'assign', 'parameters': {'final_answer': 'To achieve a reliable disaster recovery strategy for a TiDB-v4.0.14 cluster with over 50TB of data, follow these steps:\\n\\n1. **Full Backup with BR**:\\n${br_backup_plan}\\n\\n2. **Incremental Replication with TiCDC**:\\n${ticdc_replication_plan}\\n\\n3. **Garbage Collection Tuning**:\\n${gc_tuning_plan}\\n\\n4. **Restoration and Application of Changes**:\\n${restore_process}\\n\\nThis strategy ensures minimal impact on production performance and efficient storage utilization.'}}]\n",
      "Revised plan execution result {'branch_name': 're_execute_20250113_202538', 'final_answer': 'To achieve a reliable disaster recovery strategy for a TiDB-v4.0.14 cluster with over 50TB of data, follow these steps:\\n\\n1. **Full Backup with BR**:\\nTo perform a full backup of a TiDB-v4.0.14 cluster with over 50TB of data using BR (Backup & Restore), it\\'s crucial to plan carefully to minimize the impact on production performance and manage storage utilization effectively. Below is a detailed plan:\\n\\n### Backup Strategy\\n\\n1. **Schedule During Off-Peak Hours**:\\n   - Identify the off-peak hours for your production environment. This is typically during late night or early morning when user activity is minimal.\\n   - Schedule the backup to start at the beginning of this window to ensure it completes before peak hours resume.\\n\\n2. **Rate Limiting**:\\n   - Use the `--ratelimit` option to control the bandwidth used by the backup process. Set it to `100MB` as specified, which helps in reducing the load on the network and storage systems.\\n   - Monitor the impact of this rate limit and adjust if necessary to balance between backup speed and production performance.\\n\\n3. **Incremental Backups**:\\n   - Although the task is to perform a full backup, consider implementing a strategy where full backups are complemented by regular incremental backups. This reduces the data volume and time required for each backup after the initial full backup.\\n\\n### Technical Considerations\\n\\n1. **Backup Storage**:\\n   - Ensure that the backup storage location has sufficient capacity to handle over 50TB of data. Consider using cloud storage solutions that offer scalability and redundancy.\\n   - Implement data compression to reduce storage utilization. BR supports compression options that can be configured to optimize storage space.\\n\\n2. **Network Configuration**:\\n   - Ensure that the network infrastructure can handle the data transfer without affecting other operations. This might involve configuring Quality of Service (QoS) settings to prioritize critical traffic.\\n\\n3. **Cluster Configuration**:\\n   - Verify that the TiDB cluster is properly configured for backup operations. This includes ensuring that all nodes are healthy and that there are no ongoing maintenance tasks that could interfere with the backup.\\n\\n4. **Monitoring and Alerts**:\\n   - Set up monitoring to track the progress of the backup and alert on any issues such as failures or performance degradation.\\n   - Use tools like Grafana and Prometheus to visualize backup metrics and system performance.\\n\\n### Operational Steps\\n\\n1. **Preparation**:\\n   - Notify stakeholders about the scheduled backup to ensure awareness and prepare for any potential impact.\\n   - Perform a test backup on a smaller dataset to validate the backup process and configuration.\\n\\n2. **Execution**:\\n   - Initiate the backup using the BR tool with the specified `--ratelimit` option.\\n   - Monitor the backup process closely, checking for any errors or performance issues.\\n\\n3. **Post-Backup Verification**:\\n   - Verify the integrity of the backup by performing a restore on a test environment.\\n   - Check the logs for any warnings or errors that occurred during the backup process.\\n\\n4. **Documentation and Review**:\\n   - Document the backup process, including any issues encountered and resolutions.\\n   - Review the backup strategy periodically to incorporate improvements and adapt to changes in the production environment.\\n\\nBy following this plan, you can ensure a successful full backup of your TiDB cluster while minimizing the impact on production performance and optimizing storage utilization.\\n\\n2. **Incremental Replication with TiCDC**:\\nTo replicate incremental changes from a TiDB v4.0.14 cluster to a new cluster using TiCDC, you need to carefully plan and configure the replication process. Here is a detailed plan that includes considerations for minimizing network and storage impact:\\n\\n### Step 1: Environment Preparation\\n\\n1. **Ensure Compatibility**: Verify that the target cluster is compatible with TiDB v4.0.14. This includes checking the version of the target database and ensuring it supports the necessary features for replication.\\n\\n2. **Network Configuration**: Ensure that the network between the source and target clusters is reliable and has sufficient bandwidth to handle the data replication. Consider using a dedicated network link if possible to minimize latency and packet loss.\\n\\n3. **Security Setup**: Configure secure connections between the clusters. Use TLS/SSL to encrypt data in transit and ensure that only authorized users can access the replication process.\\n\\n### Step 2: TiCDC Setup\\n\\n1. **Install TiCDC**: Deploy TiCDC on a separate node or nodes to avoid resource contention with the TiDB cluster. Ensure that the TiCDC version is compatible with TiDB v4.0.14.\\n\\n2. **Configure TiCDC**: Set up TiCDC with the appropriate configuration options. Use the `--sink-uri` parameter to specify the target cluster\\'s connection details, such as:\\n   ```\\n   --sink-uri=\"mysql://user:password@target-cluster:3306/\"\\n   ```\\n\\n3. **Resource Allocation**: Allocate sufficient CPU and memory resources to TiCDC to handle the expected load. Monitor resource usage and adjust as necessary.\\n\\n### Step 3: Replication Configuration\\n\\n1. **Changefeed Creation**: Create a changefeed to specify which tables or databases to replicate. Use filters to exclude unnecessary data and reduce the amount of data transferred.\\n\\n2. **Initial Snapshot**: Decide whether to perform an initial snapshot of the data. If the target cluster already has a recent snapshot, you can skip this step to save bandwidth and storage.\\n\\n3. **Incremental Replication**: Configure TiCDC to replicate only incremental changes. This reduces the amount of data transferred and stored, minimizing network and storage impact.\\n\\n### Step 4: Monitoring and Optimization\\n\\n1. **Monitor Performance**: Continuously monitor the performance of the replication process. Use TiCDC\\'s built-in metrics and logs to identify bottlenecks or issues.\\n\\n2. **Optimize Network Usage**: If network bandwidth is a concern, consider compressing the data before transmission. TiCDC supports various compression algorithms that can be configured in the sink URI.\\n\\n3. **Storage Management**: Regularly clean up old data on the target cluster to prevent storage bloat. Implement a data retention policy that aligns with your business requirements.\\n\\n### Step 5: Testing and Validation\\n\\n1. **Test the Setup**: Before going live, test the replication setup in a staging environment. Verify that data is correctly replicated and that the system can handle the expected load.\\n\\n2. **Validation**: Perform data validation checks to ensure data consistency between the source and target clusters. Use checksums or other validation methods to confirm data integrity.\\n\\n### Step 6: Go Live\\n\\n1. **Schedule the Cutover**: Plan the cutover to the new cluster during a low-traffic period to minimize disruption.\\n\\n2. **Monitor the Transition**: Closely monitor the replication process during the transition to ensure everything is functioning as expected.\\n\\nBy following this plan, you can effectively replicate incremental changes from a TiDB v4.0.14 cluster to a new cluster using TiCDC, while minimizing network and storage impact.\\n\\n3. **Garbage Collection Tuning**:\\nTiDB is a distributed SQL database that supports MySQL-compatible syntax and is designed to handle large-scale data. One of the critical aspects of managing a TiDB cluster is tuning its garbage collection (GC) settings, especially during backup and replication processes. Proper tuning can help minimize performance impacts and optimize storage usage. Here’s a detailed plan for tuning TiDB\\'s garbage collection settings:\\n\\n### Understanding TiDB Garbage Collection\\n\\nTiDB uses a distributed garbage collection mechanism to clean up obsolete data versions. This is crucial for maintaining performance and storage efficiency, as TiDB uses a multi-version concurrency control (MVCC) model. The GC process involves:\\n\\n- **Scanning**: Identifying obsolete versions of data.\\n- **Deleting**: Physically removing these obsolete versions.\\n\\n### Key Garbage Collection Parameters\\n\\n1. **`gc_life_time`**: This parameter defines the duration for which historical data versions are retained. A longer `gc_life_time` means more historical data is kept, which can be useful for long-running transactions but increases storage usage.\\n\\n2. **`gc_run_interval`**: This determines how frequently the GC process runs. A shorter interval means more frequent cleanups, which can reduce storage usage but may impact performance.\\n\\n3. **`gc_concurrency`**: This controls the number of concurrent workers during the GC process. Higher concurrency can speed up GC but may compete with other processes for resources.\\n\\n### Tuning Strategies\\n\\n#### During Backup\\n\\n- **Increase `gc_life_time`**: Before starting a backup, increase the `gc_life_time` to ensure that no data is deleted during the backup process. This prevents data loss and ensures consistency.\\n\\n- **Schedule GC Wisely**: Adjust the `gc_run_interval` to avoid running GC during peak backup times. This can help reduce the load on the system.\\n\\n#### During Replication\\n\\n- **Balance `gc_concurrency`**: During replication, especially initial data sync, set `gc_concurrency` to a moderate level to avoid resource contention with replication processes.\\n\\n- **Monitor and Adjust**: Continuously monitor the system\\'s performance and storage usage. Adjust `gc_life_time` and `gc_run_interval` based on the observed workload and data size.\\n\\n### Workload and Data Size Considerations\\n\\n- **High Write Workloads**: For workloads with high write operations, consider reducing `gc_life_time` to free up storage space more quickly. However, ensure that this does not interfere with any long-running transactions.\\n\\n- **Large Data Volumes**: For clusters with large data volumes, increase `gc_concurrency` to speed up the GC process. Ensure that the system has enough resources to handle the increased concurrency.\\n\\n- **Low Activity Periods**: Schedule GC during periods of low activity to minimize its impact on performance. This can be done by adjusting the `gc_run_interval` to align with off-peak hours.\\n\\n### Monitoring and Feedback\\n\\n- **Use TiDB Dashboard**: Leverage the TiDB Dashboard to monitor GC activities and their impact on performance and storage. This tool provides insights into GC duration, frequency, and any potential issues.\\n\\n- **Adjust Based on Feedback**: Continuously gather feedback from the system\\'s performance metrics and adjust the GC settings accordingly. This iterative process helps in fine-tuning the settings for optimal performance.\\n\\n### Conclusion\\n\\nTuning TiDB\\'s garbage collection settings requires a careful balance between performance and storage efficiency. By adjusting parameters like `gc_life_time`, `gc_run_interval`, and `gc_concurrency`, and by considering the specific needs of backup and replication processes, you can minimize the impact of GC on your TiDB cluster. Regular monitoring and adjustments based on workload and data size are essential for maintaining an efficient and performant system.\\n\\n4. **Restoration and Application of Changes**:\\nRestoring a full backup to a new TiDB cluster and applying incremental changes using TiCDC involves several steps to ensure data integrity and availability. Here\\'s a detailed process:\\n\\n### Step 1: Prepare the New TiDB Cluster\\n\\n1. **Cluster Setup**: Deploy a new TiDB cluster with the same version as the source cluster. Ensure that the hardware and network configurations are adequate to handle the expected load.\\n\\n2. **Configuration**: Configure the new cluster with similar settings to the source cluster, including security settings, network configurations, and any necessary optimizations.\\n\\n### Step 2: Restore the Full Backup\\n\\n1. **Backup Storage Access**: Ensure that the new cluster has access to the storage location where the full backup is stored. This could be a cloud storage bucket or a network-attached storage system.\\n\\n2. **Initiate Restore**: Use the BR (Backup & Restore) tool to restore the full backup to the new cluster. The command typically looks like this:\\n   ```bash\\n   br restore full --pd \"http://new-cluster-pd:2379\" --storage \"s3://backup-bucket/path\" --ratelimit 100MB\\n   ```\\n   - **PD Address**: Replace `new-cluster-pd:2379` with the actual PD address of the new cluster.\\n   - **Storage Path**: Replace `s3://backup-bucket/path` with the actual path to the backup.\\n\\n3. **Monitor Restore Process**: Keep an eye on the restore process to ensure it completes successfully. Check logs for any errors or warnings.\\n\\n### Step 3: Set Up TiCDC for Incremental Changes\\n\\n1. **Install TiCDC**: Deploy TiCDC on a separate node or nodes to avoid resource contention. Ensure compatibility with the TiDB version.\\n\\n2. **Configure Changefeed**: Create a changefeed to capture incremental changes from the source cluster and apply them to the new cluster. Use the following command:\\n   ```bash\\n   cdc cli changefeed create --pd=http://source-cluster-pd:2379 --sink-uri=\"mysql://user:password@new-cluster:3306/\"\\n   ```\\n   - **Source PD Address**: Replace `source-cluster-pd:2379` with the actual PD address of the source cluster.\\n   - **Sink URI**: Replace `mysql://user:password@new-cluster:3306/` with the connection details of the new cluster.\\n\\n3. **Filter Configuration**: Use filters to specify which tables or databases to replicate, if necessary, to reduce unnecessary data transfer.\\n\\n### Step 4: Apply Incremental Changes\\n\\n1. **Start Replication**: Once the changefeed is configured, TiCDC will start capturing and applying incremental changes from the source cluster to the new cluster.\\n\\n2. **Monitor Replication**: Continuously monitor the replication process using TiCDC\\'s metrics and logs. Look for any lag or errors that might indicate issues.\\n\\n### Step 5: Validate Data Integrity\\n\\n1. **Data Consistency Checks**: Perform data validation checks to ensure that the data in the new cluster matches the source cluster. Use checksums or other validation methods to confirm data integrity.\\n\\n2. **Test Application**: Run application tests on the new cluster to ensure that it behaves as expected and that all data is available and consistent.\\n\\n### Step 6: Go Live\\n\\n1. **Cutover Plan**: Plan the cutover to the new cluster during a low-traffic period to minimize disruption. Ensure that all stakeholders are informed and prepared for the transition.\\n\\n2. **Final Monitoring**: After the cutover, closely monitor the new cluster to ensure it is handling the load and that there are no performance issues.\\n\\nBy following these steps, you can restore a full backup to a new TiDB cluster and apply incremental changes using TiCDC, ensuring data integrity and availability throughout the process. Regular monitoring and validation are crucial to maintaining a smooth transition and reliable system performance.\\n\\nThis strategy ensures minimal impact on production performance and efficient storage utilization.'}\n",
      "Start to evaluate plan for task(id=858e373f-09df-4e1c-8b64-e7413c4aeb2d,branch=re_execute_20250113_202538)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:26:45,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal:How can we set up a replication strategy in TiDB for backing up over 50TB of data from a TiDB-v4.0.14 cluster and restoring it to a new cluster as part of a disaster recovery strategy, using BR and TiCDC, without impacting production performance or compromising storage utilization?, the evaluation result:{'accept': False, 'answer_quality_assessment_explanation': \"The final answer does not fully resolve the goal as it lacks specific guidance on how to manage garbage collection without disabling it or significantly increasing its duration, which is a key concern for the user. Additionally, while the answer provides a comprehensive plan for backup and replication, it does not address the potential impact on production performance and storage efficiency in sufficient detail. The answer also does not include SQL examples, which could enhance understanding, as suggested by the supplementary information. This falls short of the 'Direct Problem Resolution' guideline, as it does not provide a clear and actionable solution to the user's specific problem.\", 'plan_adjustment_suggestion': \"To improve the plan, it should include specific strategies for managing garbage collection settings that align with the user's need to avoid disabling it or increasing its duration. This could involve more detailed tuning of parameters like `gc_life_time`, `gc_run_interval`, and `gc_concurrency` with examples of how these settings can be adjusted based on different workload scenarios. Additionally, the plan should provide more detailed guidance on monitoring and adjusting the backup and replication processes to ensure minimal impact on production performance and storage efficiency. Including SQL examples where relevant could also enhance the user's understanding of the processes involved.\", 'goal_classification': 'Direct Problem Resolution'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-13 20:27:22,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revised plan: [{'seq_no': 0, 'type': 'reasoning', 'parameters': {'chain_of_thoughts': 'To set up a replication strategy in TiDB for backing up over 50TB of data from a TiDB-v4.0.14 cluster and restoring it to a new cluster as part of a disaster recovery strategy, we will use BR (Backup & Restore) for full data backup and TiCDC for incremental data replication. The plan will ensure minimal impact on production performance and efficient storage utilization.\\n\\n1. **Overall Strategy**:\\n   - Use BR for full backup of the existing cluster.\\n   - Use TiCDC to capture and replicate incremental changes.\\n   - Restore the full backup to the new cluster.\\n   - Apply incremental changes to the new cluster.\\n\\n2. **Key Decision Points**:\\n   - Scheduling backups during low-traffic periods to minimize performance impact.\\n   - Configuring TiCDC to efficiently handle incremental data without overwhelming the network or storage.\\n\\n3. **Assumptions**:\\n   - The production environment can tolerate scheduled backup windows.\\n   - Network bandwidth is sufficient for TiCDC replication.\\n\\n4. **Compliance Checks**:\\n   - ✓ No user-specific queries planned (will not attempt to detect current version/configuration)\\n   - ✓ All responses will maintain consistent language (English)\\n   - ✓ Final recommendations will be stored in final_answer\\n   - ✓ All variable references use correct ${var} syntax\\n\\n5. **Expected Outcomes**:\\n   - A reliable backup and restore strategy that ensures data integrity and availability.\\n   - Minimal impact on production performance during backup and replication.\\n\\n6. **Limitations**:\\n   - The initial full backup may require significant time and resources.\\n   - Network issues could affect TiCDC replication efficiency.', 'dependency_analysis': 'Step 2 depends on Step 1. Step 3 depends on Step 2. Step 4 depends on Step 3.'}}, {'seq_no': 1, 'type': 'assign', 'parameters': {'backup_schedule': 'off-peak hours', 'br_options': '--ratelimit 100MB', 'ticdc_options': '--sink-uri=\"mysql://user:password@target-cluster:3306/\"'}}, {'seq_no': 2, 'type': 'calling', 'parameters': {'output_vars': ['br_backup_plan'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'Backup schedule: ${backup_schedule}, BR options: ${br_options}', 'prompt': 'Generate a detailed plan for using BR to perform a full backup of a TiDB-v4.0.14 cluster with over 50TB of data. Include considerations for minimizing impact on production performance and storage utilization. Please ensure that the generated text uses English.'}}}, {'seq_no': 3, 'type': 'calling', 'parameters': {'output_vars': ['ticdc_replication_plan'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'TiCDC options: ${ticdc_options}', 'prompt': 'Generate a detailed plan for using TiCDC to replicate incremental changes from a TiDB-v4.0.14 cluster to a new cluster. Include considerations for minimizing network and storage impact. Please ensure that the generated text uses English.'}}}, {'seq_no': 4, 'type': 'calling', 'parameters': {'output_vars': ['gc_tuning_plan'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'TiDB garbage collection settings and their impact on performance and storage', 'prompt': 'Provide a detailed plan for tuning garbage collection settings in TiDB to minimize impact during backup and replication processes. Include strategies for adjusting parameters like `gc_life_time`, `gc_run_interval`, and `gc_concurrency` based on workload and data size. Please ensure that the generated text uses English.'}}}, {'seq_no': 5, 'type': 'calling', 'parameters': {'output_vars': ['restore_process'], 'tool_name': 'llm_generate', 'tool_params': {'context': 'BR backup plan: ${br_backup_plan}, TiCDC replication plan: ${ticdc_replication_plan}, GC tuning plan: ${gc_tuning_plan}', 'prompt': 'Describe the process of restoring a full backup to a new TiDB cluster and applying incremental changes using TiCDC. Ensure the process maintains data integrity and availability. Please ensure that the generated text uses English.'}}}, {'seq_no': 6, 'type': 'assign', 'parameters': {'final_answer': 'To achieve a reliable disaster recovery strategy for a TiDB-v4.0.14 cluster with over 50TB of data, follow these steps:\\n\\n1. **Full Backup with BR**:\\n${br_backup_plan}\\n\\n2. **Incremental Replication with TiCDC**:\\n${ticdc_replication_plan}\\n\\n3. **Garbage Collection Tuning**:\\n${gc_tuning_plan}\\n\\n4. **Restoration and Application of Changes**:\\n${restore_process}\\n\\nThis strategy ensures minimal impact on production performance and efficient storage utilization.'}}]\n",
      "Revised plan execution result {'branch_name': 're_execute_20250113_202723', 'final_answer': 'To achieve a reliable disaster recovery strategy for a TiDB-v4.0.14 cluster with over 50TB of data, follow these steps:\\n\\n1. **Full Backup with BR**:\\nTo perform a full backup of a TiDB-v4.0.14 cluster with over 50TB of data using BR (Backup & Restore), it\\'s crucial to plan carefully to minimize the impact on production performance and manage storage utilization effectively. Below is a detailed plan:\\n\\n### 1. **Preparation**\\n\\n- **Understand the Environment**: Ensure you have a clear understanding of your TiDB cluster\\'s architecture, including the number of nodes, network bandwidth, and storage capacity.\\n- **Backup Storage**: Choose a reliable and scalable storage solution for the backup data. Options include Amazon S3, Google Cloud Storage, or a high-capacity NAS.\\n- **Network Configuration**: Ensure that the network can handle the data transfer rate without affecting the production workload. Consider using a dedicated network for backup operations if possible.\\n\\n### 2. **Scheduling**\\n\\n- **Off-Peak Hours**: Schedule the backup during off-peak hours to minimize the impact on production performance. Analyze your cluster\\'s usage patterns to identify the best time window.\\n- **Incremental Backups**: Although this plan focuses on a full backup, consider implementing incremental backups in the future to reduce the load and storage requirements.\\n\\n### 3. **Backup Configuration**\\n\\n- **Rate Limiting**: Use the `--ratelimit` option to control the data transfer rate. Set it to `100MB` to ensure that the backup process does not saturate the network bandwidth or overwhelm the storage system.\\n  \\n  ```shell\\n  br backup full --pd ${PD_ADDRESS} --storage ${BACKUP_STORAGE} --ratelimit 100MB\\n  ```\\n\\n- **Concurrency**: Adjust the concurrency settings to balance between speed and resource usage. Start with a moderate level and adjust based on observed performance.\\n\\n### 4. **Execution**\\n\\n- **Monitoring**: Continuously monitor the backup process to ensure it is running smoothly. Use TiDB\\'s monitoring tools to track the impact on CPU, memory, and network usage.\\n- **Error Handling**: Implement error handling and retry mechanisms to address any issues that arise during the backup process.\\n\\n### 5. **Post-Backup Verification**\\n\\n- **Data Integrity**: Verify the integrity of the backup data by performing checksum validation.\\n- **Restore Testing**: Periodically test the restore process to ensure that the backup can be successfully restored in case of a disaster.\\n\\n### 6. **Optimization and Maintenance**\\n\\n- **Review and Optimize**: After the initial backup, review the process and optimize settings based on the observed impact and performance.\\n- **Regular Maintenance**: Schedule regular maintenance windows to update and optimize the backup system, including software updates and configuration adjustments.\\n\\n### 7. **Documentation and Training**\\n\\n- **Documentation**: Maintain detailed documentation of the backup process, including configuration settings, schedules, and procedures for both backup and restore operations.\\n- **Training**: Ensure that the relevant team members are trained on the backup and restore procedures to handle any issues that may arise.\\n\\nBy following this plan, you can effectively perform a full backup of your TiDB cluster while minimizing the impact on production performance and managing storage utilization efficiently.\\n\\n2. **Incremental Replication with TiCDC**:\\nTo replicate incremental changes from a TiDB v4.0.14 cluster to a new cluster using TiCDC, you need to carefully plan and configure the replication process. Here is a detailed plan that includes considerations for minimizing network and storage impact:\\n\\n### Step 1: Environment Preparation\\n\\n1. **Ensure Compatibility**: Verify that the target cluster is compatible with TiDB v4.0.14. This includes checking the version of the target database and ensuring it supports the necessary features for replication.\\n\\n2. **Network Configuration**: Ensure that the network between the source and target clusters is reliable and has sufficient bandwidth to handle the data replication. Consider using a dedicated network link if possible to minimize latency and packet loss.\\n\\n3. **Security Setup**: Configure secure connections between the clusters. Use TLS/SSL to encrypt data in transit and ensure that only authorized users can access the replication process.\\n\\n### Step 2: TiCDC Setup\\n\\n1. **Install TiCDC**: Deploy TiCDC on a separate node or nodes to avoid resource contention with the TiDB cluster. Ensure that the TiCDC version is compatible with TiDB v4.0.14.\\n\\n2. **Configure Sink URI**: Use the `--sink-uri` option to specify the target cluster\\'s connection details. For example:\\n   ```\\n   --sink-uri=\"mysql://user:password@target-cluster:3306/\"\\n   ```\\n   Replace `user`, `password`, and `target-cluster` with the appropriate credentials and address of the target cluster.\\n\\n3. **Create Changefeed**: Set up a changefeed to start capturing changes from the source cluster. Use the `cdc cli` to create a changefeed with appropriate settings:\\n   ```bash\\n   cdc cli changefeed create --sink-uri=\"mysql://user:password@target-cluster:3306/\"\\n   ```\\n\\n### Step 3: Optimization for Network and Storage\\n\\n1. **Filter Unnecessary Data**: Use TiCDC\\'s filtering capabilities to exclude tables or databases that do not need to be replicated. This reduces the amount of data transferred over the network.\\n\\n2. **Batch Size and Compression**: Adjust the batch size for data transfer to optimize network usage. Enable compression if supported to reduce the data size.\\n\\n3. **Monitor and Adjust**: Continuously monitor the replication process using TiCDC\\'s monitoring tools. Adjust configurations based on network performance and storage usage.\\n\\n### Step 4: Testing and Validation\\n\\n1. **Initial Testing**: Before starting full replication, perform a test run with a subset of data to ensure that the setup works as expected.\\n\\n2. **Data Consistency Check**: After the initial replication, verify data consistency between the source and target clusters. Use checksums or other validation methods to ensure data integrity.\\n\\n3. **Performance Monitoring**: Monitor the performance of both the source and target clusters during replication. Look for any bottlenecks or issues that could impact performance.\\n\\n### Step 5: Ongoing Maintenance\\n\\n1. **Regular Updates**: Keep TiCDC and TiDB updated with the latest patches and versions to ensure optimal performance and security.\\n\\n2. **Backup and Recovery**: Implement a backup and recovery strategy for both the source and target clusters to protect against data loss.\\n\\n3. **Scalability Planning**: As data volume grows, plan for scaling the TiCDC setup to handle increased load without impacting performance.\\n\\nBy following this plan, you can effectively replicate incremental changes from a TiDB v4.0.14 cluster to a new cluster while minimizing network and storage impact.\\n\\n3. **Garbage Collection Tuning**:\\nTo effectively tune garbage collection (GC) settings in TiDB, especially during backup and replication processes, it\\'s crucial to understand how these settings impact performance and storage. The primary parameters to consider are `gc_life_time`, `gc_run_interval`, and `gc_concurrency`. Here\\'s a detailed plan for tuning these settings:\\n\\n### 1. Understanding the Parameters\\n\\n- **`gc_life_time`**: This parameter defines the duration for which historical data is retained before being eligible for garbage collection. A longer `gc_life_time` means more historical data is kept, which can be useful for long-running transactions but increases storage usage.\\n\\n- **`gc_run_interval`**: This setting determines how frequently the garbage collection process runs. A shorter interval means more frequent GC cycles, which can help in reclaiming space quickly but may increase the load on the system.\\n\\n- **`gc_concurrency`**: This parameter controls the number of concurrent workers during the GC process. Higher concurrency can speed up the GC process but may also lead to increased resource contention.\\n\\n### 2. Tuning Strategy\\n\\n#### A. Assess Workload and Data Size\\n\\n1. **Analyze Workload Patterns**: Determine if your workload is read-heavy, write-heavy, or balanced. This will influence how aggressively you can tune the GC settings.\\n\\n2. **Evaluate Data Size and Growth**: Understand the current data size and expected growth. Larger datasets may require more conservative GC settings to avoid performance degradation.\\n\\n#### B. Adjust `gc_life_time`\\n\\n- **For Backup and Replication**: During backup and replication, consider extending the `gc_life_time` to ensure that historical data is available if needed. This is particularly important if backups or replication lag behind.\\n\\n- **Post-Backup/Replication**: Once the backup or replication process is complete, you can reduce the `gc_life_time` to reclaim storage space.\\n\\n#### C. Configure `gc_run_interval`\\n\\n- **During Peak Operations**: Increase the `gc_run_interval` to reduce the frequency of GC operations, minimizing their impact on system performance.\\n\\n- **During Off-Peak Hours**: Decrease the `gc_run_interval` to allow more frequent GC cycles, which can help in reclaiming space without affecting peak performance.\\n\\n#### D. Set `gc_concurrency`\\n\\n- **Resource Availability**: If your system has spare CPU and I/O capacity, increase `gc_concurrency` to speed up the GC process.\\n\\n- **System Load**: If the system is under heavy load, reduce `gc_concurrency` to prevent resource contention.\\n\\n### 3. Monitoring and Iteration\\n\\n- **Monitor Performance Metrics**: Continuously monitor TiDB performance metrics such as CPU usage, I/O wait times, and transaction latencies to assess the impact of GC settings.\\n\\n- **Iterate Based on Feedback**: Use the insights gained from monitoring to iteratively adjust the GC settings. Aim for a balance between performance and storage efficiency.\\n\\n### 4. Best Practices\\n\\n- **Stagger GC Operations**: If possible, schedule GC operations during periods of low activity to minimize their impact on performance.\\n\\n- **Test Changes in a Staging Environment**: Before applying changes to production, test them in a staging environment to understand their impact.\\n\\n- **Document Changes**: Keep a record of all changes made to GC settings and their observed effects to inform future tuning efforts.\\n\\nBy carefully tuning these parameters based on your specific workload and data characteristics, you can minimize the impact of garbage collection on TiDB performance and storage during backup and replication processes.\\n\\n4. **Restoration and Application of Changes**:\\nRestoring a full backup to a new TiDB cluster and applying incremental changes using TiCDC involves several steps to ensure data integrity and availability. Here\\'s a detailed process:\\n\\n### Step 1: Prepare the New TiDB Cluster\\n\\n1. **Cluster Setup**: Deploy a new TiDB cluster with the same version as the source cluster. Ensure that the hardware and network configurations are adequate to handle the data load.\\n\\n2. **Configuration**: Configure the new cluster with similar settings to the source cluster, including security settings, network configurations, and storage options.\\n\\n### Step 2: Restore the Full Backup\\n\\n1. **Backup Storage Access**: Ensure that the new cluster has access to the storage location where the full backup is stored. This could be an S3 bucket, Google Cloud Storage, or a NAS.\\n\\n2. **Restore Process**: Use the BR (Backup & Restore) tool to restore the full backup to the new cluster. Execute the following command, replacing placeholders with actual values:\\n\\n   ```shell\\n   br restore full --pd ${NEW_PD_ADDRESS} --storage ${BACKUP_STORAGE}\\n   ```\\n\\n   - **Monitoring**: Continuously monitor the restore process to ensure it completes successfully without errors. Use TiDB\\'s monitoring tools to track resource usage and performance.\\n\\n3. **Data Integrity Check**: After the restore is complete, perform a data integrity check to ensure that the data in the new cluster matches the backup. Use checksums or other validation methods to verify data consistency.\\n\\n### Step 3: Set Up TiCDC for Incremental Changes\\n\\n1. **Install TiCDC**: Deploy TiCDC on a separate node or nodes to avoid resource contention with the new TiDB cluster. Ensure compatibility with the TiDB version.\\n\\n2. **Configure Sink URI**: Set up the sink URI to point to the new cluster. This configuration will direct TiCDC to apply changes to the new cluster.\\n\\n   ```shell\\n   --sink-uri=\"mysql://user:password@new-cluster:3306/\"\\n   ```\\n\\n3. **Create Changefeed**: Use the `cdc cli` to create a changefeed that captures changes from the source cluster and applies them to the new cluster:\\n\\n   ```bash\\n   cdc cli changefeed create --sink-uri=\"mysql://user:password@new-cluster:3306/\"\\n   ```\\n\\n   - **Filtering**: If necessary, configure filters to exclude certain tables or databases that do not need to be replicated.\\n\\n### Step 4: Apply Incremental Changes\\n\\n1. **Start Replication**: Once the changefeed is set up, TiCDC will start capturing and applying incremental changes from the source cluster to the new cluster.\\n\\n2. **Monitor Replication**: Continuously monitor the replication process to ensure it is running smoothly. Use TiCDC\\'s monitoring tools to track latency, throughput, and any potential errors.\\n\\n3. **Data Consistency Verification**: Periodically verify data consistency between the source and new clusters. Use checksums or other validation methods to ensure that all changes are accurately applied.\\n\\n### Step 5: Final Validation and Cutover\\n\\n1. **Final Consistency Check**: Before switching to the new cluster, perform a final data consistency check to ensure that all data is synchronized.\\n\\n2. **Cutover**: Once you are confident in the data integrity and availability of the new cluster, redirect application traffic to the new cluster.\\n\\n3. **Post-Cutover Monitoring**: After the cutover, continue to monitor the new cluster for any performance issues or data discrepancies.\\n\\nBy following these steps, you can restore a full backup to a new TiDB cluster and apply incremental changes using TiCDC while maintaining data integrity and availability.\\n\\nThis strategy ensures minimal impact on production performance and efficient storage utilization.'}\n"
     ]
    }
   ],
   "source": [
    "from app.controller.label_classifier import LabelClassifier\n",
    "\n",
    "\n",
    "# optimize_plan(\"0eda1580-95db-4219-a23b-58336c3b8f4e\", \"main\")\n",
    "\n",
    "classifier = LabelClassifier()\n",
    "\n",
    "end_time = datetime.utcnow()\n",
    "start_time = end_time - timedelta(hours=2)\n",
    "\n",
    "pending_tasks = get_evaluation_pending_tasks(\n",
    "    start_time=start_time\n",
    ")\n",
    "\n",
    "for task in pending_tasks:\n",
    "    task_id = task[\"id\"]\n",
    "    optimize_plan(task_id, \"main\")\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: spec.md file not found at /Users/ian/Work/stackvm/notebooks/spec.md\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add the project root to Python path\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "from app.services.llm_interface import LLMInterface\n",
    "from app.config.settings import LLM_MODEL, LLM_PROVIDER\n",
    "\n",
    "stackvm_host = os.getenv(\"STACKVM_HOST\", None)\n",
    "assert stackvm_host is not None, \"STACKVM_HOST environment variable is not set.\"\n",
    "\n",
    "def get_task_branch_answer_detail(task_id: str, branch_name: Optional[str]=\"main\") -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the answer detail for a specific task and branch using the API.\n",
    "\n",
    "    Args:\n",
    "        task_id: The ID of the task.\n",
    "        branch_name: The name of the branch.\n",
    "        base_url: The base URL of the API.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the API response, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    url = f\"{stackvm_host}/api/tasks/{task_id}/branches/{branch_name}/answer_detail\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request: {e}\")\n",
    "        raise e\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response: {e}\")\n",
    "        raise e\n",
    "\n",
    "def re_execute_task(\n",
    "    task_id: str,\n",
    "    plan: List[Dict[str, Any]]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Updates a task with a new suggestion and sets the task to be re-run from scratch.\n",
    "    \"\"\"\n",
    "    url = f\"{stackvm_host}/api/tasks/{task_id}/re_execute\"\n",
    "    \n",
    "    payload = {\n",
    "        \"plan\": plan,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if response.status_code == 400:\n",
    "            raise ValueError(\"Missing required parameters\")\n",
    "        elif response.status_code == 404:\n",
    "            raise ValueError(f\"Task with ID {task_id} not found\")\n",
    "        elif response.status_code == 500:\n",
    "            raise ValueError(\"Failed to re-execute task\")\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "\n",
    "llm_client = LLMInterface(LLM_PROVIDER, LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_task(goal, answer, plan):\n",
    "    evluation_prompt = f\"\"\"You are tasked with evaluating and improving the effectiveness of a problem-solving workflow. Below is a description of a Goal, a Plan used to address it, and the Final Answer generated. Your task is to evaluate the quality of the answer and diagnose whether the plan sufficiently aligned with the goal. If issues are present (e.g., the answer does not fully meet the goal or contains irrelevant information), you must:\n",
    "1. Analyze the Plan to identify weaknesses or misalignments with the Goal.\n",
    "2. Provide detailed suggestions to adjust or rewrite the Plan to improve the answer quality.\n",
    "\n",
    "Your output must include:\n",
    "1. Answer Quality Assessment: Clearly state whether the final answer resolves the goal. If not, explain why and identify any irrelevant or missing elements.\n",
    "2. Plan Analysis: Examine the steps in the plan, identify where they failed or could be improved, and explain why adjustments are necessary.\n",
    "3. Plan Adjustment Suggestions: Provide a revised or improved version of the plan to address the identified shortcomings.\n",
    "\n",
    "Here are the inputs:\n",
    "\n",
    "## Goal \n",
    "{goal}\n",
    "\n",
    "## Answer\n",
    "{answer}\n",
    "\n",
    "## plan\n",
    "{plan}\n",
    "\n",
    "Your Output Format:\n",
    "You must return a JSON object with the following keys:\n",
    "- accept: Boolean value (true or false) indicating whether the final answer effectively resolves the goal.\n",
    "- answer_quality_assessment_explaination: A detailed explanation justifying why the final answer does or does not meet the goal, highlighting key points or missing elements.\n",
    "- plan_adjustment_suggestion: If answer is not accepted, please provide a comprehensive analysis of the plan and recommendations for how to adjust or improve it to better achieve the goal.\n",
    "\n",
    "Example Output:\n",
    "{{\n",
    "  \"accept\": False/True,\n",
    "  \"answer_quality_assessment_explaination\": \"...\",\n",
    "  \"plan_adjustment_suggestion\": {...}\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    return llm_client.generate(prompt=evluation_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_plan(goal, answer, plan, adjustment_suggestion):\n",
    "    \"\"\"\n",
    "    Generates a new plan based on the evaluation results. It modifies only the necessary parts\n",
    "    of the original plan to address the identified issues.\n",
    "\n",
    "    Parameters:\n",
    "    - goal (str): The original goal.\n",
    "    - answer (str): The final answer generated.\n",
    "    - plan (str): The original plan used to achieve the goal.\n",
    "    - adjustment_suggestion (str): The plan adjustment suggestions from the evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - new_plan (str): The revised plan addressing the evaluation feedback.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Craft a prompt to generate the new plan based on the suggestions\n",
    "    new_plan_prompt = f\"\"\"You are tasked with revising an existing plan to better achieve a specified goal based on evaluation feedback. The revisions should be minimal, only addressing the issues identified without overhauling the entire plan.\n",
    "\n",
    "## Goal:\n",
    "{goal}\n",
    "\n",
    "## Original Plan:\n",
    "{plan}\n",
    "\n",
    "## Original Answer:\n",
    "{answer}\n",
    "\n",
    "## Evaluation Feedback:\n",
    "{adjustment_suggestion}\n",
    "\n",
    "## Requirements for the New Plan:\n",
    "- Modify only the necessary parts of the original plan.\n",
    "- Incorporate the suggestions from the evaluation feedback.\n",
    "- Ensure the revised plan is coherent and aligned with the goal.\n",
    "- **Information Retrieval Enhancement:** When performing information retrieval, use both knowledge graph search and vector search to ensure the richness of retrieved information. Note that knowledge graph search is a powerful retrieval function.\n",
    "- **Selective Plan Modification:** If parts of the original answer meet the expected outcomes, identify and retain the corresponding information retrieval steps from the original plan. This approach ensures that only necessary modifications are made, preventing unpredictable performance fluctuations in the revised plan.\n",
    "\n",
    "Now, let's update the plan.\n",
    "\n",
    "**Output**:\n",
    "1. Provide the complete updated plan in JSON format, ensuring it adheres to the VM specification.\n",
    "2. Provide a summary of the changes made to the plan, including a diff with the previous plan.\n",
    "\"\"\"\n",
    "\n",
    "    # Generate the new plan using the LLM\n",
    "    new_plan = llm_client.generate(prompt=new_plan_prompt)\n",
    "\n",
    "    return new_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_evaluation_pending_tasks(        \n",
    "    start_time: Optional[datetime] = None,\n",
    "    end_time: Optional[datetime] = None\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetches the list of tasks pending evaluation from the API.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API (e.g., 'http://stackvm-dev.tidb.ai:5556').\n",
    "        start_time (Optional[datetime]): The start time to filter tasks.\n",
    "        end_time (Optional[datetime]): The end time to filter tasks.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: A list of tasks pending evaluation.\n",
    "    \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the request fails.\n",
    "        ValueError: If the response cannot be decoded.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{stackvm_host}/api/tasks/evaluation_pending\"\n",
    "    params = {}\n",
    "    \n",
    "    if start_time:\n",
    "        params['start_time'] = start_time.isoformat()\n",
    "    if end_time:\n",
    "        params['end_time'] = end_time.isoformat()\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(endpoint, params=params)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses (4XX or 5XX)\n",
    "        data = response.json()\n",
    "        \n",
    "        if not isinstance(data, list):\n",
    "            raise ValueError(\"Unexpected response format: Expected a list of tasks.\")\n",
    "        \n",
    "        return data\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        # Handle network-related errors\n",
    "        print(f\"An error occurred while making the request: {e}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        # Handle JSON decoding errors or unexpected data formats\n",
    "        print(f\"An error occurred while processing the response: {ve}\")\n",
    "        raise\n",
    "\n",
    "def record_evaluation(\n",
    "    task_id: str,\n",
    "    evaluation_status: str,\n",
    "    evaluation_reason: Optional[str] = \"\",\n",
    "    timeout: int = 10\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Records the evaluation result of a specific task by calling the API endpoint.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the API (e.g., 'http://stackvm-dev.tidb.ai:5556').\n",
    "        task_id (str): The ID of the task to be evaluated.\n",
    "        evaluation_status (str): The evaluation status (e.g., \"APPROVED\", \"REJECTED\").\n",
    "        evaluation_reason (Optional[str]): The reason for the evaluation decision.\n",
    "        api_token (Optional[str]): API token for authentication, if required.\n",
    "        timeout (int): Timeout in seconds for the API request.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The JSON response from the API indicating success or failure.\n",
    "    \n",
    "    Raises:\n",
    "        requests.exceptions.RequestException: If the request fails.\n",
    "        ValueError: If the response cannot be decoded or contains an error.\n",
    "    \"\"\"\n",
    "    endpoint = f\"{stackvm_host}/api/tasks/{task_id}/evaluation\"\n",
    "    payload = {\n",
    "        \"evaluation_status\": evaluation_status,\n",
    "        \"evaluation_reason\": evaluation_reason\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(endpoint, json=payload, headers=headers, timeout=timeout)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        if not isinstance(data, dict):\n",
    "            raise ValueError(\"Unexpected response format: Expected a JSON object.\")\n",
    "\n",
    "        if not data.get(\"success\", False):\n",
    "            error_message = data.get(\"error\", \"Unknown error occurred.\")\n",
    "            raise ValueError(f\"API Error: {error_message}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred while making the request: {e}\")\n",
    "        raise\n",
    "    except ValueError as ve:\n",
    "        print(f\"An error occurred while processing the response: {ve}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to evaluate plan for task(id=0340dd64-41c2-452a-a6ae-587f698a3126,branch=main)\n",
      "{'accept': False, 'answer_quality_assessment_explaination': \"The final answer does not effectively resolve the goal. The goal was to answer the question 'Who are you?' with a clear self-identification of the AI assistant. However, the provided answer is a generic response that does not address the question directly. It lacks specific information about the identity and capabilities of the AI assistant, which were intended to be included according to the plan.\", 'plan_adjustment_suggestion': {'Plan Analysis': \"The plan intended to use the LLM to generate a response that includes the identity and capabilities of the AI assistant. However, the final answer did not reflect this intention. The plan's execution failed to ensure that the generated response was aligned with the goal. The prompt used in the plan was appropriate, but the final step did not correctly assign the generated content to the final answer.\", 'Plan Adjustment Suggestions': [{'parameters': {'chain_of_thoughts': \"To answer the question 'Who are you?', we will directly use the LLM to generate a response. The goal is to provide a clear and concise self-identification of the AI assistant, including its identity and capabilities. This approach ensures that the response is accurate and consistent with the intended language and format.\", 'dependency_analysis': 'This plan consists of a single step, as the task is straightforward and does not require additional data retrieval or complex processing.'}, 'seq_no': 0, 'type': 'reasoning'}, {'parameters': {'output_vars': ['identity_and_capabilities'], 'tool_name': 'llm_generate', 'tool_params': {'context': None, 'prompt': 'You are StackVM, a task-solving Agent developed by PingCAP, proficient in planning for users’ needs, selecting appropriate tools to orchestrate solutions to various problems users have. Your primary function is to assist users by understanding and generating human-like text based on the input you receive. You can help answer questions, provide explanations, generate creative content, and assist with a wide range of informational needs. Please ensure that the generated text uses English.'}}, 'seq_no': 1, 'type': 'calling'}, {'parameters': {'final_answer': '${identity_and_capabilities}'}, 'seq_no': 2, 'type': 'assign'}], 'Revised Plan': [{'parameters': {'chain_of_thoughts': \"To answer the question 'Who are you?', we will directly use the LLM to generate a response. The goal is to provide a clear and concise self-identification of the AI assistant, including its identity and capabilities. This approach ensures that the response is accurate and consistent with the intended language and format.\", 'dependency_analysis': 'This plan consists of a single step, as the task is straightforward and does not require additional data retrieval or complex processing.'}, 'seq_no': 0, 'type': 'reasoning'}, {'parameters': {'output_vars': ['identity_and_capabilities'], 'tool_name': 'llm_generate', 'tool_params': {'context': None, 'prompt': 'You are StackVM, a task-solving Agent developed by PingCAP, proficient in planning for users’ needs, selecting appropriate tools to orchestrate solutions to various problems users have. Your primary function is to assist users by understanding and generating human-like text based on the input you receive. You can help answer questions, provide explanations, generate creative content, and assist with a wide range of informational needs. Please ensure that the generated text uses English.'}}, 'seq_no': 1, 'type': 'calling'}, {'parameters': {'final_answer': '${identity_and_capabilities}'}, 'seq_no': 2, 'type': 'assign'}]}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from app.utils.json import extract_json\n",
    "\n",
    "pending_tasks = get_evaluation_pending_tasks()\n",
    "\n",
    "for task in pending_tasks:\n",
    "    task_id = task[\"id\"]\n",
    "    print(f\"Start to evaluate plan for task(id={task_id},branch=main)\")\n",
    "    detail = get_task_branch_answer_detail(task_id)\n",
    "\n",
    "    state = detail.get('vm_state')\n",
    "    goal_completed = False\n",
    "    final_answer = None\n",
    "    plan = None\n",
    "    goal = None\n",
    "\n",
    "    if state is not None:\n",
    "        plan = state.get(\"current_plan\", None)\n",
    "        goal_completed = state.get(\"goal_completed\", False)\n",
    "        goal = state.get(\"goal\", None)\n",
    "        if state.get(\"variables\", None) is not None:\n",
    "            final_answer = state['variables'].get(\"final_answer\", None)\n",
    "\n",
    "        if goal is not None and goal_completed is True and plan is not None and final_answer is not None:\n",
    "            response = evaluation_task(goal, final_answer, plan)\n",
    "            eval_res_str = extract_json(response)\n",
    "            eval_res = json.loads(eval_res_str)\n",
    "\n",
    "            eval_status = \"APPROVED\" if eval_res.get(\"accept\", False) else \"REJECTED\"\n",
    "            eval_reason = json.dumps(eval_res) \n",
    "            print(eval_res)\n",
    "\n",
    "            record_evaluation(task_id, eval_status, eval_reason)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

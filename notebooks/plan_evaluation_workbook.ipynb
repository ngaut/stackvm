{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: spec.md file not found at /Users/ian/Work/stackvm/notebooks/spec.md\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add the project root to Python path\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "from app.services.llm_interface import LLMInterface\n",
    "from app.config.settings import LLM_MODEL, LLM_PROVIDER\n",
    "\n",
    "stackvm_host = os.getenv(\"STACKVM_HOST\", None)\n",
    "assert stackvm_host is not None, \"STACKVM_HOST environment variable is not set.\"\n",
    "\n",
    "def get_task_branch_answer_detail(task_id: str, branch_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the answer detail for a specific task and branch using the API.\n",
    "\n",
    "    Args:\n",
    "        task_id: The ID of the task.\n",
    "        branch_name: The name of the branch.\n",
    "        base_url: The base URL of the API.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the API response, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    url = f\"{stackvm_host}/api/tasks/{task_id}/branches/{branch_name}/answer_detail\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request: {e}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "llm_client = LLMInterface(LLM_PROVIDER, LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_task(goal, answer, plan):\n",
    "    evluation_prompt = f\"\"\"You are tasked with evaluating and improving the effectiveness of a problem-solving workflow. Below is a description of a Goal, a Plan used to address it, and the Final Answer generated. Your task is to evaluate the quality of the answer and diagnose whether the plan sufficiently aligned with the goal. If issues are present (e.g., the answer does not fully meet the goal or contains irrelevant information), you must:\n",
    "1. Analyze the Plan to identify weaknesses or misalignments with the Goal.\n",
    "2. Provide detailed suggestions to adjust or rewrite the Plan to improve the answer quality.\n",
    "\n",
    "Your output must include:\n",
    "1. Answer Quality Assessment: Clearly state whether the final answer resolves the goal. If not, explain why and identify any irrelevant or missing elements.\n",
    "2. Plan Analysis: Examine the steps in the plan, identify where they failed or could be improved, and explain why adjustments are necessary.\n",
    "3. Plan Adjustment Suggestions: Provide a revised or improved version of the plan to address the identified shortcomings.\n",
    "\n",
    "Here are the inputs:\n",
    "\n",
    "## Goal \n",
    "{goal}\n",
    "\n",
    "## Answer\n",
    "{answer}\n",
    "\n",
    "## plan\n",
    "{plan}\n",
    "\n",
    "Your Output Format:\n",
    "You must return a JSON object with the following keys:\n",
    "- accept: Boolean value (true or false) indicating whether the final answer effectively resolves the goal.\n",
    "- answer_quality_assessment_explaination: A detailed explanation justifying why the final answer does or does not meet the goal, highlighting key points or missing elements.\n",
    "- plan_adjustment_suggestion: If answer is not accepted, please provide a comprehensive analysis of the plan and recommendations for how to adjust or improve it to better achieve the goal.\n",
    "\n",
    "Example Output:\n",
    "{{\n",
    "  \"accept\": False/True,\n",
    "  \"answer_quality_assessment_explaination\": \"...\",\n",
    "  \"plan_adjustment_suggestion\": {...}\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    return llm_client.generate(prompt=evluation_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to evaluate plan for task(id=fe605c06-1fc5-47d8-a728-25f1a025befd,branch=main)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from app.utils.json import extract_json\n",
    "\n",
    "\n",
    "# task_id = 'c3382869-e2b2-4244-b971-d00a14701681'\n",
    "task_id = 'fe605c06-1fc5-47d8-a728-25f1a025befd'\n",
    "branch_name = 'main'\n",
    "\n",
    "print(f\"Start to evaluate plan for task(id={task_id},branch={branch_name})\")\n",
    "\n",
    "detail = get_task_branch_answer_detail(task_id, branch_name)\n",
    "state = detail.get('vm_state')\n",
    "goal_completed = False\n",
    "final_answer = None\n",
    "plan = None\n",
    "goal = None\n",
    "\n",
    "if state is not None:\n",
    "    plan = state.get(\"current_plan\", None)\n",
    "    goal_completed = state.get(\"goal_completed\", False)\n",
    "    goal = state.get(\"goal\", None)\n",
    "    if state.get(\"variables\", None) is not None:\n",
    "        final_answer = state['variables'].get(\"final_answer\", None)\n",
    "\n",
    "    if goal is not None and goal_completed is True and plan is not None and final_answer is not None:\n",
    "        response = evaluation_task(goal, final_answer, plan)\n",
    "        eval_res_str = extract_json(response)\n",
    "        eval_res = json.loads(eval_res_str)\n",
    "        accept = eval_res.get(\"accept\", None)\n",
    "        if accept is not None and not accept:\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accept': True,\n",
       " 'answer_quality_assessment_explaination': 'The final answer effectively resolves the goal by providing a comprehensive list of potential reasons why TiDB Drainer might fail to start, along with corresponding solutions for each identified issue. The answer is well-structured and covers a range of possible causes, including configuration issues, resource limitations, network problems, and version compatibility. It also provides actionable solutions, such as using specific tools and checking configurations, which are relevant and practical for addressing the startup issues. There are no irrelevant elements, and the answer is complete in addressing the goal.',\n",
       " 'plan_adjustment_suggestion': None}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

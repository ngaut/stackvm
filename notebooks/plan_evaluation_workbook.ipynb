{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: spec.md file not found at /Users/ianz/Work/stackvm/notebooks/spec.md\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")  # Add the project root to Python path\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from app.services.llm_interface import LLMInterface\n",
    "from app.config.settings import LLM_MODEL, LLM_PROVIDER\n",
    "\n",
    "stackvm_host = os.getenv(\"STACKVM_HOST\", None)\n",
    "assert stackvm_host is not None, \"STACKVM_HOST environment variable is not set.\"\n",
    "\n",
    "def get_task_branch_answer_detail(task_id: str, branch_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the answer detail for a specific task and branch using the API.\n",
    "\n",
    "    Args:\n",
    "        task_id: The ID of the task.\n",
    "        branch_name: The name of the branch.\n",
    "        base_url: The base URL of the API.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the API response, or None if an error occurred.\n",
    "    \"\"\"\n",
    "    url = f\"{stackvm_host}/api/tasks/{task_id}/branches/{branch_name}/answer_detail\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during request: {e}\")\n",
    "        raise e\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON response: {e}\")\n",
    "        raise e\n",
    "\n",
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "def re_execute_task(\n",
    "    task_id: str,\n",
    "    plan: List[Dict[str, Any]]\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Updates a task with a new suggestion and sets the task to be re-run from scratch.\n",
    "    \"\"\"\n",
    "    url = f\"{stackvm_host}/api/tasks/{task_id}/re_execute\"\n",
    "    \n",
    "    payload = {\n",
    "        \"plan\": plan,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if response.status_code == 400:\n",
    "            raise ValueError(\"Missing required parameters\")\n",
    "        elif response.status_code == 404:\n",
    "            raise ValueError(f\"Task with ID {task_id} not found\")\n",
    "        elif response.status_code == 500:\n",
    "            raise ValueError(\"Failed to re-execute task\")\n",
    "        else:\n",
    "            raise e\n",
    "    \n",
    "\n",
    "llm_client = LLMInterface(LLM_PROVIDER, LLM_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_task(goal, answer, plan):\n",
    "    evluation_prompt = f\"\"\"You are tasked with evaluating and improving the effectiveness of a problem-solving workflow. Below is a description of a Goal, a Plan used to address it, and the Final Answer generated. Your task is to evaluate the quality of the answer and diagnose whether the plan sufficiently aligned with the goal. If issues are present (e.g., the answer does not fully meet the goal or contains irrelevant information), you must:\n",
    "1. Analyze the Plan to identify weaknesses or misalignments with the Goal.\n",
    "2. Provide detailed suggestions to adjust or rewrite the Plan to improve the answer quality.\n",
    "\n",
    "Your output must include:\n",
    "1. Answer Quality Assessment: Clearly state whether the final answer resolves the goal. If not, explain why and identify any irrelevant or missing elements.\n",
    "2. Plan Analysis: Examine the steps in the plan, identify where they failed or could be improved, and explain why adjustments are necessary.\n",
    "3. Plan Adjustment Suggestions: Provide a revised or improved version of the plan to address the identified shortcomings.\n",
    "\n",
    "Here are the inputs:\n",
    "\n",
    "## Goal \n",
    "{goal}\n",
    "\n",
    "## Answer\n",
    "{answer}\n",
    "\n",
    "## plan\n",
    "{plan}\n",
    "\n",
    "Your Output Format:\n",
    "You must return a JSON object with the following keys:\n",
    "- accept: Boolean value (true or false) indicating whether the final answer effectively resolves the goal.\n",
    "- answer_quality_assessment_explaination: A detailed explanation justifying why the final answer does or does not meet the goal, highlighting key points or missing elements.\n",
    "- plan_adjustment_suggestion: If answer is not accepted, please provide a comprehensive analysis of the plan and recommendations for how to adjust or improve it to better achieve the goal.\n",
    "\n",
    "Example Output:\n",
    "{{\n",
    "  \"accept\": False/True,\n",
    "  \"answer_quality_assessment_explaination\": \"...\",\n",
    "  \"plan_adjustment_suggestion\": {...}\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    return llm_client.generate(prompt=evluation_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_plan(goal, answer, plan, adjustment_suggestion):\n",
    "    \"\"\"\n",
    "    Generates a new plan based on the evaluation results. It modifies only the necessary parts\n",
    "    of the original plan to address the identified issues.\n",
    "\n",
    "    Parameters:\n",
    "    - goal (str): The original goal.\n",
    "    - answer (str): The final answer generated.\n",
    "    - plan (str): The original plan used to achieve the goal.\n",
    "    - adjustment_suggestion (str): The plan adjustment suggestions from the evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - new_plan (str): The revised plan addressing the evaluation feedback.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Craft a prompt to generate the new plan based on the suggestions\n",
    "    new_plan_prompt = f\"\"\"You are tasked with revising an existing plan to better achieve a specified goal based on evaluation feedback. The revisions should be minimal, only addressing the issues identified without overhauling the entire plan.\n",
    "\n",
    "## Goal:\n",
    "{goal}\n",
    "\n",
    "## Original Plan:\n",
    "{plan}\n",
    "\n",
    "## Original Answer:\n",
    "{answer}\n",
    "\n",
    "## Evaluation Feedback:\n",
    "{adjustment_suggestion}\n",
    "\n",
    "## Requirements for the New Plan:\n",
    "- Modify only the necessary parts of the original plan.\n",
    "- Incorporate the suggestions from the evaluation feedback.\n",
    "- Ensure the revised plan is coherent and aligned with the goal.\n",
    "- **Information Retrieval Enhancement:** When performing information retrieval, use both knowledge graph search and vector search to ensure the richness of retrieved information. Note that knowledge graph search is a powerful retrieval function.\n",
    "- **Selective Plan Modification:** If parts of the original answer meet the expected outcomes, identify and retain the corresponding information retrieval steps from the original plan. This approach ensures that only necessary modifications are made, preventing unpredictable performance fluctuations in the revised plan.\n",
    "\n",
    "Now, let's update the plan.\n",
    "\n",
    "**Output**:\n",
    "1. Provide the complete updated plan in JSON format, ensuring it adheres to the VM specification.\n",
    "2. Provide a summary of the changes made to the plan, including a diff with the previous plan.\n",
    "\"\"\"\n",
    "\n",
    "    # Generate the new plan using the LLM\n",
    "    new_plan = llm_client.generate(prompt=new_plan_prompt)\n",
    "\n",
    "    return new_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to evaluate plan for task(id=5575fe37-6b39-493a-929a-42112416a86f,branch=main)\n",
      "Update plan based on the suggestion: {'Plan Analysis': 'The plan has several weaknesses. First, it does not clearly separate the tasks of configuring the `region-split-size` and verifying its persistence. The inclusion of unrelated configuration examples suggests a lack of focus in the information retrieval and generation steps. Additionally, the plan does not ensure that the generated content is concise and directly relevant to the goal.', 'Plan Adjustment Suggestions': ['1. Refine the initial reasoning step to emphasize the specific goal of ensuring `region-split-size` persistence, avoiding unrelated configuration examples.', '2. In the information retrieval steps (seq_no 1 and 2), focus queries specifically on `region-split-size` persistence, rather than general configuration persistence.', '3. In the LLM generation step (seq_no 3), ensure the prompt explicitly requests steps for configuring and verifying `region-split-size` persistence, and instructs to avoid unrelated examples.', '4. Consolidate the configuration and verification steps into a single coherent narrative that directly addresses the goal, ensuring clarity and relevance.', '5. Remove any unrelated configuration examples from the final answer to maintain focus on the `region-split-size`.']}\n",
      "Explanation: The final answer does not effectively resolve the goal. While it provides steps to ensure the `region-split-size` setting persists after a node restart, it includes irrelevant information about modifying other system variables like `tidb_slow_log_threshold`, which is not related to the goal. Additionally, the answer is somewhat repetitive and lacks clarity in distinguishing between configuration and verification steps. The answer should focus solely on the `region-split-size` and its persistence, without introducing unrelated configuration examples.\n",
      "Updated plan: Here is the updated plan in JSON format, followed by a summary of the changes made:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"parameters\": {\n",
      "            \"chain_of_thoughts\": \"To ensure that changes to the region-split-size in TiDB persist after a node restart, we need to focus specifically on configuring the region-split-size and verifying its persistence. The steps include: 1. Retrieve information on how to configure the region-split-size to persist across restarts. 2. Identify the specific configuration file or method to set the region-split-size. 3. Verify the changes are correctly applied and persist after a restart. This approach ensures that the configuration is durable and effective.\",\n",
      "            \"dependency_analysis\": \"Step 2 depends on the information gathered in Step 1. Step 3 verifies the implementation of Step 2.\"\n",
      "        },\n",
      "        \"seq_no\": 0,\n",
      "        \"type\": \"reasoning\"\n",
      "    },\n",
      "    {\n",
      "        \"parameters\": {\n",
      "            \"output_vars\": [\"persistence_info\"],\n",
      "            \"tool_name\": \"vector_search\",\n",
      "            \"tool_params\": {\n",
      "                \"query\": \"How to configure region-split-size in TiDB to persist across restarts?\",\n",
      "                \"top_k\": 3\n",
      "            }\n",
      "        },\n",
      "        \"seq_no\": 1,\n",
      "        \"type\": \"calling\"\n",
      "    },\n",
      "    {\n",
      "        \"parameters\": {\n",
      "            \"output_vars\": [\"persistence_info_kg\"],\n",
      "            \"tool_name\": \"retrieve_knowledge_graph\",\n",
      "            \"tool_params\": {\n",
      "                \"query\": \"How to configure region-split-size in TiDB to persist across restarts?\"\n",
      "            }\n",
      "        },\n",
      "        \"seq_no\": 2,\n",
      "        \"type\": \"calling\"\n",
      "    },\n",
      "    {\n",
      "        \"parameters\": {\n",
      "            \"output_vars\": [\"configuration_and_verification_steps\"],\n",
      "            \"tool_name\": \"llm_generate\",\n",
      "            \"tool_params\": {\n",
      "                \"context\": \"${persistence_info} ${persistence_info_kg}\",\n",
      "                \"prompt\": \"根据以下信息，提供如何在 TiDB 中配置和验证 region-split-size 以确保其在节点重启后持久化的步骤。请确保生成的文本使用中文，并避免不相关的示例。\"\n",
      "            }\n",
      "        },\n",
      "        \"seq_no\": 3,\n",
      "        \"type\": \"calling\"\n",
      "    },\n",
      "    {\n",
      "        \"parameters\": {\n",
      "            \"final_answer\": \"要确保 TiDB 中的 region-split-size 更改在节点重启后持久化，请按照以下步骤进行：\\n1. 配置和验证步骤：${configuration_and_verification_steps}\"\n",
      "        },\n",
      "        \"seq_no\": 4,\n",
      "        \"type\": \"assign\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "### Summary of Changes:\n",
      "\n",
      "1. **Refined Reasoning Step (seq_no 0):** \n",
      "   - Focused the reasoning on configuring and verifying the persistence of `region-split-size` specifically, avoiding unrelated configuration examples.\n",
      "\n",
      "2. **Information Retrieval Steps (seq_no 1 and 2):**\n",
      "   - Adjusted the queries to specifically target `region-split-size` persistence rather than general configuration persistence.\n",
      "\n",
      "3. **LLM Generation Step (seq_no 3):**\n",
      "   - Modified the prompt to explicitly request steps for configuring and verifying `region-split-size` persistence, instructing to avoid unrelated examples.\n",
      "\n",
      "4. **Consolidation of Steps:**\n",
      "   - Combined configuration and verification steps into a single coherent narrative to directly address the goal, ensuring clarity and relevance.\n",
      "\n",
      "5. **Final Answer (seq_no 4):**\n",
      "   - Removed unrelated configuration examples to maintain focus on `region-split-size`.\n",
      "\n",
      "These changes address the evaluation feedback by ensuring the plan is concise, focused, and directly relevant to the goal of ensuring `region-split-size` persistence in TiDB.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from app.utils.json import extract_json\n",
    "\n",
    "task_id = '5575fe37-6b39-493a-929a-42112416a86f'\n",
    "# task_id = 'c3382869-e2b2-4244-b971-d00a14701681'\n",
    "# task_id = 'fe605c06-1fc5-47d8-a728-25f1a025befd'\n",
    "branch_name = 'main'\n",
    "\n",
    "print(f\"Start to evaluate plan for task(id={task_id},branch={branch_name})\")\n",
    "\n",
    "detail = get_task_branch_answer_detail(task_id, branch_name)\n",
    "\n",
    "state = detail.get('vm_state')\n",
    "goal_completed = False\n",
    "final_answer = None\n",
    "plan = None\n",
    "goal = None\n",
    "updated_plan = None\n",
    "\n",
    "if state is not None:\n",
    "    plan = state.get(\"current_plan\", None)\n",
    "    goal_completed = state.get(\"goal_completed\", False)\n",
    "    goal = state.get(\"goal\", None)\n",
    "    if state.get(\"variables\", None) is not None:\n",
    "        final_answer = state['variables'].get(\"final_answer\", None)\n",
    "\n",
    "    if goal is not None and goal_completed is True and plan is not None and final_answer is not None:\n",
    "        response = evaluation_task(goal, final_answer, plan)\n",
    "        eval_res_str = extract_json(response)\n",
    "        eval_res = json.loads(eval_res_str)\n",
    "        accept = eval_res.get(\"accept\", None)\n",
    "\n",
    "        if accept is not True:\n",
    "            explanation = eval_res.get(\"answer_quality_assessment_explaination\", None)\n",
    "            suggestion = eval_res.get(\"plan_adjustment_suggestion\", None)\n",
    "\n",
    "            if suggestion is not None:\n",
    "                print(f\"Update plan based on the suggestion: {suggestion}\")\n",
    "                print(f\"Explanation: {explanation}\")\n",
    "                updated_plan_response = generate_new_plan(goal, final_answer, plan, suggestion)\n",
    "                print(f\"Updated plan: {updated_plan_response}\")\n",
    "                updated_plan_str = extract_json(updated_plan_response)\n",
    "                updated_plan = json.loads(updated_plan_str)\n",
    "            else:\n",
    "                print(\"No suggestion found\")\n",
    "        else:\n",
    "            print(\"The final answer is accepted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_res = re_execute_task(task_id, updated_plan)\n",
    "updated_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stackvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

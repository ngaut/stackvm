{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
    "!pip install python-dotenv\n",
    "!pip install huggingface_hub\n",
    "!pip install wandb\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/unsloth/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "login(hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/zhaiyl/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhaiyl\u001b[0m (\u001b[33mpingcap\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zhaiyl/finetune/wandb/run-20250221_093701-q5yta1v4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pingcap/Fine-tune-DeepSeek-R1-Distill-Qwen-7B%20on%20Stack%20Experiment%20Dataset/runs/q5yta1v4?apiKey=dd16e9906f3d2827395836f36fe2cca6445e0f03' target=\"_blank\">fearless-disco-14</a></strong> to <a href='https://wandb.ai/pingcap/Fine-tune-DeepSeek-R1-Distill-Qwen-7B%20on%20Stack%20Experiment%20Dataset?apiKey=dd16e9906f3d2827395836f36fe2cca6445e0f03' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pingcap/Fine-tune-DeepSeek-R1-Distill-Qwen-7B%20on%20Stack%20Experiment%20Dataset?apiKey=dd16e9906f3d2827395836f36fe2cca6445e0f03' target=\"_blank\">https://wandb.ai/pingcap/Fine-tune-DeepSeek-R1-Distill-Qwen-7B%20on%20Stack%20Experiment%20Dataset?apiKey=dd16e9906f3d2827395836f36fe2cca6445e0f03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pingcap/Fine-tune-DeepSeek-R1-Distill-Qwen-7B%20on%20Stack%20Experiment%20Dataset/runs/q5yta1v4?apiKey=dd16e9906f3d2827395836f36fe2cca6445e0f03' target=\"_blank\">https://wandb.ai/pingcap/Fine-tune-DeepSeek-R1-Distill-Qwen-7B%20on%20Stack%20Experiment%20Dataset/runs/q5yta1v4?apiKey=dd16e9906f3d2827395836f36fe2cca6445e0f03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wb_token = os.environ.get(\"WANDB\")\n",
    "\n",
    "wandb.login(key=wb_token)\n",
    "run = wandb.init(\n",
    "    project='Fine-tune-DeepSeek-R1-Distill-Qwen-7B on Stack Experiment Dataset', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.1.8: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA A100-SXM4-40GB. Max memory: 39.381 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.48s/it]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 8192 \n",
    "dtype = None \n",
    "load_in_4bit = False\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-7B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    token = hf_token, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "def describe_goal(goal: str, metadata: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Describe the goal in a more detailed and structured way.\n",
    "\n",
    "    Args:\n",
    "        goal: The original task goal\n",
    "        metadata: Task metadata containing response format, label path etc.\n",
    "\n",
    "    Returns:\n",
    "        A formatted string describing the complete goal context\n",
    "    \"\"\"\n",
    "    description_parts = []\n",
    "\n",
    "    # Add the main goal\n",
    "    description_parts.append(f\"Goal: {goal}\")\n",
    "\n",
    "    if metadata:\n",
    "        response_format = metadata.get(\"response_format\", {})\n",
    "\n",
    "        # Add background information if present\n",
    "        background = response_format.get(\"Background\") or response_format.get(\n",
    "            \"background\"\n",
    "        )\n",
    "        if background:\n",
    "            description_parts.append(f\"Background: {background}\")\n",
    "\n",
    "        # Add annotations if present\n",
    "        annotations = response_format.get(\"Annotations\") or response_format.get(\n",
    "            \"annotations\"\n",
    "        )\n",
    "        if annotations:\n",
    "            description_parts.append(f\"Annotations: {annotations}\")\n",
    "\n",
    "        # Add language information if present\n",
    "        lang = response_format.get(\"Lang\") or response_format.get(\"lang\")\n",
    "        if lang:\n",
    "            description_parts.append(f\"Response Language: {lang}\")\n",
    "\n",
    "        # Add format requirements if present\n",
    "        format_req = response_format.get(\"Format\") or response_format.get(\"format\")\n",
    "        if format_req:\n",
    "            description_parts.append(f\"Response Format: {format_req}\")\n",
    "\n",
    "        # Add label path if present\n",
    "        label_path = metadata.get(\"label_path\")\n",
    "        if label_path:\n",
    "            if isinstance(label_path, list):\n",
    "                # Handle both string list and dict list formats\n",
    "                path_str = \" -> \".join(\n",
    "                    item[\"label\"] if isinstance(item, dict) else item\n",
    "                    for item in label_path\n",
    "                )\n",
    "                description_parts.append(f\"Labels: {path_str}\")\n",
    "\n",
    "    return \"\\n\".join(description_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Your task is to generate a detailed action plan to achieve the following goal:\n",
    "\n",
    "Goal: {}\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "**MUST follow the Specification**:\n",
    "# Specification for Generating Executable Plans for the Virtual Machine (VM)\n",
    "\n",
    "## Table of Contents\n",
    "1. Overview of the VM\n",
    "2. Instruction Format\n",
    "3. Supported Instructions\n",
    "4. Parameters and Variable References\n",
    "5. Variables and Dependencies\n",
    "6. Plan Structure\n",
    "7. Best Practices\n",
    "8. Common Errors\n",
    "9. Available Tools for calling instruction\n",
    "10. Example Plan\n",
    "\n",
    "## 1. Overview of the VM\n",
    "The VM executes plans consisting of a sequence of instructions. Each instruction performs a specific operation and may interact with variables stored in a variable store. The VM supports conditional execution and can handle dependencies between instructions through variable assignments and references.\n",
    "\n",
    "### Key features:\n",
    "- **Variable Store**: A key-value store where variables are stored and accessed by name.\n",
    "- **Instruction Execution**: Instructions are executed sequentially unless control flow is altered by conditional statements.\n",
    "\n",
    "## 2. Instruction Format\n",
    "Each instruction in the plan is represented as a JSON object with the following keys:\n",
    "\n",
    "- `seq_no`: A unique and AUTO-INCREMENT integer identifying the instruction's sequence within the plan, starting from 0.\n",
    "- `type`: A string indicating the instruction type. See Supported Instructions.\n",
    "- `parameters`: An object containing parameters required by the instruction.\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": N,\n",
    "  \"type\": \"instruction_type\",\n",
    "  \"parameters\": {{\n",
    "    \"param1\": \"value_or_variable_reference\",\n",
    "    \"param2\": \"value_or_variable_reference\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## 3. Supported Instructions\n",
    "### 3.1 assign\n",
    "- **Purpose**: Assigns values to one or more variables.\n",
    "- **Parameters**: An object where each key is a variable name. Each value can be:\n",
    "  1. A direct value (number/string).\n",
    "  2. A reference to an existing variable: use the syntax \"${{variable_name}}\".\n",
    "  3. A template string that interpolates variables for string concatenation.\n",
    "     - Example: \"The reason is: ${{reason}}, and the solution is: ${{solution}}\"\n",
    "  4. A basic arithmetic expression involving numeric variables:\n",
    "     - Supported operators: +, -, *, /, ** (pow), % (mod), unary +/-\n",
    "     - Example: \"${{var0}} / 3 + ${{var1}}\"\n",
    "\n",
    "The VM will:\n",
    "1. Replace each \"${{varName}}\" with the current value of varName.\n",
    "2. If the result is a pure numeric expression (e.g., 2+3, 5*6, or referencing numeric variables), it will be evaluated as a number.\n",
    "3. If the result is a string with placeholders, it becomes a string concatenation or template filling.\n",
    "4. Assign the final computed result back to the target variable(s).\n",
    "\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. Direct Assignment\n",
    "   ```json\n",
    "   {{\n",
    "     \"seq_no\": 0,\n",
    "     \"type\": \"assign\",\n",
    "     \"parameters\": {{\n",
    "       \"constant_number\": 42,\n",
    "       \"message\": \"Hello World\"\n",
    "     }}\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "2. Template/String Interpolation\n",
    "   ```json\n",
    "   {{\n",
    "     \"seq_no\": 1,\n",
    "     \"type\": \"assign\",\n",
    "     \"parameters\": {{\n",
    "       \"recommended_solution\": \"Reason: ${{reason}}\\nSolution: ${{solution}}\"\n",
    "     }}\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "3. Basic Arithmetic\n",
    "   ```json\n",
    "   {{\n",
    "     \"seq_no\": 2,\n",
    "     \"type\": \"assign\",\n",
    "     \"parameters\": {{\n",
    "       \"calculated_result\": \"${{num1}} + ${{num2}} / 3\"\n",
    "     }}\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "### 3.2 jmp\n",
    "- **Purpose**: Jumps to a specified sequence number based on an optional condition.\n",
    "- **Parameters**:\n",
    "  - `condition_prompt` (optional): The prompt to evaluate the condition. If provided, the LLM evaluates whether to jump. **Must respond with a JSON object in the following format:**\n",
    "    ```json\n",
    "    {{\n",
    "      \"result\": boolean,\n",
    "      \"explanation\": string\n",
    "    }}\n",
    "    ```\n",
    "  - `context` (optional): Additional context for the LLM. Can be a direct string or a variable reference.\n",
    "  - `jump_if_true`: The `seq_no` to jump to if the condition evaluates to true. Required if `condition_prompt` is provided.\n",
    "  - `jump_if_false` (optional): The `seq_no` to jump to if the condition evaluates to false. Required if `condition_prompt` is provided.\n",
    "  - `target_seq` (optional): The `seq_no` to jump to if no condition is provided (unconditional jump).\n",
    "\n",
    "**Example (Conditional Jump):**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 4,\n",
    "  \"type\": \"jmp\",\n",
    "  \"parameters\": {{\n",
    "    \"condition_prompt\": \"Is ${{number}} even? Respond with a JSON object in the following format:\\n{{\\n  \\\"result\\\": boolean,\\n  \\\"explanation\\\": string\\n}}\\nWhere 'result' is true if the number is even, false otherwise, and 'explanation' provides a brief reason for the result.\",\n",
    "    \"context\": null,\n",
    "    \"jump_if_true\": 5,\n",
    "    \"jump_if_false\": 6\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**Example (Unconditional Jump):**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 5,\n",
    "  \"type\": \"jmp\",\n",
    "  \"parameters\": {{\n",
    "    \"target_seq\": 7\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 3.3 calling\n",
    "- **Purpose**: Invokes a specific tool or function with the provided parameters.\n",
    "- **Parameters**: Defines the specifications required to call a tool.\n",
    "  - `tool_name`: The name of the tool to be called for `calling` instruction.\n",
    "  - `tool_params`: An object containing key-value pairs that represent the arguments required by the specified tool.\n",
    "    - Keys: Must match the argument names expected by the tool.\n",
    "    - Values: Can be either a direct value or a variable reference.\n",
    "  - `output_vars` (optional): An array specifying how the tool's output should be stored in the VM's variable store for later use.\n",
    "    - If it is a string: The array contains one variable name. The entire tool's response is stored under this variable name.\n",
    "    - If it is an array: The array contains variable names corresponding to the keys in the JSON response. Each variable name in the array maps to a key in the JSON object, and the value associated with each key will be extracted and stored under the corresponding variable name.\n",
    "\n",
    "The structure of `calling` instruction:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": <unique_sequential_number>,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"<tool_name>\",\n",
    "    \"tool_params\": {{\n",
    "      <tool-specific parameters>\n",
    "    }},\n",
    "    \"output_vars\": [<list_of_output_variable_names>]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 1,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"tool_name\",\n",
    "    \"tool_params\": {{\n",
    "      \"param1\": \"value_or_variable_reference\",\n",
    "      \"param2\": \"value_or_variable_reference\"\n",
    "    }},\n",
    "    \"output_vars\": [\"variable_name_1\", ...]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## 4. Parameters and Variable References\n",
    "Parameters can be either direct values or variable references. To reference a variable, use the format `${{variable_name}}`.\n",
    "\n",
    "- **Direct Values** are used when you clearly know the corresponding parameter values. These values do not depend on the results of other instructions, ensuring clarity and simplicity. Using direct values helps improve query readability and maintainability, especially in scenarios where parameters do not need to change dynamically.\n",
    "\n",
    "- **Variable References** are ideal for scenarios that require dynamic parameter value filling, enhancing the interconnectivity and data flow between instructions. By using variable references, parameters can be adjusted dynamically based on the results of previous steps, increasing the flexibility and automation of the workflow.\n",
    "\n",
    "- **Don't Use Math Expressions in Parameters and tool_params**: The VM does not have the capability to compute or parse expressions within parameters. It can only perform simple reference substitutions. For example, avoid using expressions like value1 + value2 or value * 2 within parameters, and instead, calculate these values explicitly in a prior step and refer to the result in the parameter.\n",
    "\n",
    "\n",
    "**Direct Value Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 1,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"retrieve_knowledge_graph\",\n",
    "    \"tool_params\": {{\n",
    "      \"query\": \"TiDB latest stable version\"\n",
    "    }},\n",
    "    \"output_vars\": [\"latest_tidb_version_info\"]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**Variable Reference Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 4,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"vector_search\",\n",
    "    \"tool_params\": {{\n",
    "      \"query\": \"What are the key features and improvements in TiDB version ${{latest_stable_tidb_version}}?\",\n",
    "      \"top_k\": 10\n",
    "    }},\n",
    "    \"output_vars\": [\"tidb_key_features_and_improvements\"]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## 5. Variables and Dependencies\n",
    "- **Variable Assignment**: Use the `assign` instruction or specify an `output_vars` in a `calling` instruction that produces outputs.\n",
    "- **Variable Access**: Reference variables in parameters using the variable reference format.\n",
    "- **Dependencies**: Manage dependencies by assigning outputs to variables and referencing them in subsequent instructions.\n",
    "\n",
    "## 6. Plan Structure\n",
    "- **Sequential Execution**: Instructions are executed in order based on their `seq_no`.\n",
    "- **Control Flow**: Use the `jmp` instruction for branching logic and conditional loops.\n",
    "\n",
    "## 7. Best Practices\n",
    "- **Sequence Numbering**: Ensure that `seq_no` values are unique, sequential integers within the plan.\n",
    "- **Variable Naming**: Use descriptive variable names to make the plan readable and maintainable.\n",
    "- **Control Flow**: Use `jmp` instructions to create conditional logic, manage execution flow, and implement loops effectively.\n",
    "- **Final answer**: The name of output var of The last instruction MUST be \"final_answer\".\n",
    "- **Language Consistency**: All the instructions (e.g. `llm_generate`) that directly contribute to generating the `final_answer` must be written in the same language as the Response Language (if not specified, use the same language of the goal). This ensures the final output is consistent with the intended language.\n",
    "\n",
    "- **Instruction type selection**: Available instruction types:[assign, jmp, calling].\n",
    "\n",
    "- **Avoid variable dependencies within a single \"assign\" instruction**：Since the order of variable assignments within an \"assign\" instruction is not defined, do not rely on one variable being assigned before another within the same instruction. Instead, split assignments across multiple instructions if one depends on another. For example, this is incorrect:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 3,\n",
    "  \"type\": \"assign\",\n",
    "  \"parameters\": {{\n",
    "    \"y\": \"${{x}}\",\n",
    "    \"x\": 10\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "\"y\" might end up being undefined because we cannot guarantee that \"x\" will be set first. The correct approach is to split them:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 3,\n",
    "  \"type\": \"assign\",\n",
    "  \"parameters\": {{\n",
    "    \"x\": 10\n",
    "  }}\n",
    "}},\n",
    "{{\n",
    "  \"seq_no\": 4,\n",
    "  \"type\": \"assign\",\n",
    "  \"parameters\": {{\n",
    "    \"y\": \"${{x}}\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "- **Best Practices for Information Retrieval - Combining Knowledge Graph Search and Vector Search**:\n",
    "  - Dual Retrieval: When retrieving information, utilize both Knowledge Graph Search and Vector Search simultaneously. This combination enhances the richness of the information by leveraging the structured data from the knowledge graph and the detailed insights from vector search.\n",
    "  - Unified Summarization: After retrieving data from both tools, use an LLM generation tool to summarize the knowledge related to the query. Avoid directly using the loose data returned by the two tools; instead, ensure all retrieved information is processed through the LLM generation tool to create a coherent and well-structured final answer.\n",
    "  - Tool Integration: Ensure that raw data retrieved from both Knowledge Graph Search and Vector Search is exclusively processed by the LLM generation tool. Do not pass this data to other tools, as doing so may result in an unreadable final answer or prevent other tools from effectively processing the data. This practice maintains the coherence, integrity, and quality of the final response.\n",
    "  - Maintain Coherence: By processing all retrieved data through the LLM generation tool, you ensure that the final answer is a cohesive, single-language narrative. This avoids the inclusion of raw or fragmented data that could compromise the readability and consistency of the response.\n",
    "\n",
    "- **Final Answer Alignment**:\n",
    "  - **Goal-Centric Generation**: Ensure that the generated `final_answer` directly addresses the question or objective outlined in the goal. The `final_answer` should be focused and relevant to the goal and avoid general response.\n",
    "  - **Contextual Consistency**: Since the tools in the plan (e.g., `llm_generate`) do not aware the goal, include the goal context when making tool calls if necessary. Maintain the alignment between the goal and all intermediate steps leading to the `final_answer`. This ensures that every instruction and tool interaction contributes towards achieving the desired outcome.\n",
    "  - **Avoid Divergence**: Prevent the generation of information that, while relevant, does not serve to answer the primary goal. All synthesized and summarized data should reinforce the goal-centric `final_answer`.\n",
    "\n",
    "## 8. Common Errors\n",
    "\n",
    "**Case 1: Querying Specific Runtime/Environment Information**\n",
    "\n",
    "**Error Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 1,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"tool_name\",\n",
    "    \"tool_params\": {{\n",
    "      \"query\": \"Determin the current version of ...\"\n",
    "    }},\n",
    "    \"output_vars\": [...]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"parameters\": {{\n",
    "    \"output_vars\": [\n",
    "      \"slow_query_log_explanation\",\n",
    "      \"sample_slow_query_log\"\n",
    "    ],\n",
    "    \"tool_name\": \"llm_generate\",\n",
    "    \"tool_params\": {{\n",
    "      \"context\": null,\n",
    "      \"prompt\": \"Please analyze the sql query: `SELECT * FROM INFORMATION_SCHEMA.SLOW_QUERY ORDER BY start_time DESC LIMIT 10;`. Explain the slow query and its relevant details(at least contain 'query', 'start_time', 'duration', 'plan_digest').\\n\\nPlease ensure that the generated text uses English.\"\n",
    "    }}\n",
    "  }},\n",
    "  \"seq_no\": 2,\n",
    "  \"type\": \"calling\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Error Explanation**:\n",
    "\n",
    "- **Not allowed to execute SQL**: Please do not use any tools, such as llm_generate, to attempt to obtain SQL execution results.\n",
    "- **Do Not Assume Specific Environment Information**: Do not make assumptions about (or generate) specific details of the environment, such as their current system configuration, current versions of tidb, current tiup version, or private data. Plans should be designed to be adaptable and not rely on presumed specific environment information.\n",
    "- **Avoid Obtain Specific Data with General Tools**: General tools like `retrieve_knowledge_graph`, `vector_search` and `llm_generate` can only access public documentation and general knowledge. They cannot access:\n",
    "  - Current system configuration\n",
    "  - Current version\n",
    "  - Cluster status\n",
    "  - Any private or runtime information\n",
    "  Such specific environment information can only be obtained through specialized tools explicitly designed for that purpose, or should be provided by the user as part of their query.\n",
    "\n",
    "\n",
    "## 9. Available Tools for `calling` instruction\n",
    "\n",
    "\n",
    "Please use only the following tools in Calling Instruction:\n",
    "\n",
    "### retrieve_knowledge_graph\n",
    "\n",
    "\n",
    "    Retrieves TiDB related information from a knowledge graph, returning nodes and relationships between those nodes.\n",
    "\n",
    "    This tool is designed to extract structured knowledge about TiDB from a knowledge graph. It excels at identifying entities and relationships, providing a rich context of interconnected information.\n",
    "\n",
    "    Arguments:\n",
    "      - `query`: The query string. This should be a question or statement about TiDB entities, concepts, or their relationships. Can be a direct string or a variable reference.\n",
    "\n",
    "    Output:\n",
    "      - Returns a single dictionary (`Dict`) representing the retrieved knowledge graph data. This dictionary contains a complex structure representing nodes and the relationships between them, extracted from the knowledge graph. \n",
    "        **Important:** The raw output of this tool, a complex dictionary representing graph data, is **not intended for direct use in the final answer.**  The knowledge graph data is returned in a structured format that requires further processing to be presented in a user-friendly and coherent manner.\n",
    "\n",
    "\n",
    "    Best practices:\n",
    "    - **Prioritize for Information Retrieval:** For most information retrieval tasks related to TiDB knowledge, the `retrieve_knowledge_graph` tool should be your **first choice**.  Consider using `retrieve_knowledge_graph` and `vector_search` together with the **same query** to retrieve complementary information and increase the richness of results.  `vector_search` can be used as a secondary option when graph-based knowledge is insufficient.\n",
    "    - **Refine and Synthesize with `llm_generate`:** After retrieving information using `retrieve_knowledge_graph` (and optionally `vector_search`), **always** process the raw output using the `llm_generate` tool.  Use `llm_generate` to refine, summarize, and synthesize the retrieved knowledge graph data (and document snippets if using `vector_search` as well) into a concise and user-friendly answer. Do **not** directly use the raw output in the `final_answer`.\n",
    "    - **Focus Queries on General TiDB Knowledge:**  Target your queries towards general, shared knowledge about TiDB concepts and relationships.  Avoid queries that are specific to a user's environment or seek private data like configurations or versions, which is out of scope of this tool.\n",
    "    \n",
    "\n",
    "### llm_generate\n",
    "\n",
    "\n",
    "    Generates a response using the Language Model (LLM).\n",
    "\n",
    "    This tool must be used within a \"calling\" instruction in the plan.\n",
    "\n",
    "    Arguments:\n",
    "    - `prompt`: The prompt to provide to the LLM. Can be a direct string or a variable reference.\n",
    "        - **Language Matching**: Write the prompt in the same language as the goal.\n",
    "        - **Language Confirmation**: Append a sentence to confirm the desired language of the generated text:\n",
    "            - *For English goals*: \"Please ensure that the generated text uses English.\"\n",
    "            - *For Chinese goals*: \"请确保生成的文本使用中文。\"\n",
    "            - *For Japanese goals*: \"Please ensure that the generated text uses Japanese.\"\n",
    "    - `context` (optional): Additional context for the LLM. Can be a direct string or a variable reference.\n",
    "\n",
    "    Output: The output format (text or JSON) depends on your instructions.\n",
    "    - Text Response: If you ask for a text answer, let output_vars be an array containing one variable name. The entire text response will be stored under this variable.\n",
    "    - JSON Response: If you instruct the LLM to respond in JSON format, let output_vars be an array containing variable names that match the keys in the JSON response. Each variable name corresponds to a key in the JSON object, and the value associated with each key is stored under the corresponding variable name.\n",
    "\n",
    "    Example usage in a plan:\n",
    "    ```json\n",
    "    {{\n",
    "        \"seq_no\": 1,\n",
    "        \"type\": \"calling\",\n",
    "        \"parameters\": {{\n",
    "            \"tool_name\": \"llm_generate\",\n",
    "            \"tool_params\": {{\n",
    "                \"prompt\": \"Analyze the sales data and provide summary and insights, response a json object including keys ['summary', 'insights'].\",\n",
    "                \"context\": \"${{sales_data}}\"\n",
    "            }},\n",
    "            \"output_vars\": [\"summary\", \"insights\"]\n",
    "        }}\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    Best practices:\n",
    "    - Always use llm_generate within a \"calling\" instruction in your plan.\n",
    "    - Use variable references (${{variable_name}}) when you need to include dynamic content from previous steps.\n",
    "    \n",
    "\n",
    "### vector_search\n",
    "\n",
    "\n",
    "    Retrieves the most relevant snippets of TiDB documentation based on embedding similarity to your query.\n",
    "\n",
    "    This tool leverages vector embeddings to find document fragments from TiDB documentation that are most semantically similar to your query. It excels at finding relevant document snippets that provide rich context and detailed information.\n",
    "\n",
    "    Arguments:\n",
    "      - `query`: The query string. It should be a clear and simple statement or question, focusing on a single objective for best results.\n",
    "      - `top_k`: The number of top document snippets to retrieve. Must be an integer or a variable referencing an integer.\n",
    "\n",
    "    Output:\n",
    "      - Returns a list of dictionaries (`List[Dict]`). Each dictionary represents a retrieved document chunk and contains information about the chunk (e.g., content, source). **Important:** The raw output of this tool, a list of dictionaries, is **not intended for direct use in the final answer.** The document chunks are returned as individual fragments and require further processing to form a coherent response.\n",
    "\n",
    "\n",
    "    Example to call this tool:\n",
    "\n",
    "    **Example:**\n",
    "    ```json\n",
    "    {{\n",
    "        \"seq_no\": 3,\n",
    "        \"type\": \"calling\",\n",
    "        \"parameters\": {{\n",
    "            \"tool_name\": \"vector_search\",\n",
    "            \"tool_params\": {{\n",
    "                \"query\": \"Information about ...\",\n",
    "                \"top_k\": 10\n",
    "            }},\n",
    "            \"output_vars\": [\"embedded_chunks\"]\n",
    "        }}\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    Best practices:\n",
    "      - **Process Output with `llm_generate`:**  The `vector_search` tool returns a list of document chunks. **Always** process this raw output using the `llm_generate` tool to summarize, synthesize, and refine the information into a coherent answer before using it in the final response.  Do **not** directly use the raw `vector_search` output in the `final_answer`.\n",
    "      - **Use Clear, Focused Queries:** For the best search results, ensure your query is clear, concise, and focuses on a **single**, specific question or objective. Avoid multi-part or ambiguous queries.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Now, let's generate the plan.\n",
    "\n",
    "1. **Analyze the Request**:\n",
    "   - Determine the primary intent behind the goal.\n",
    "   - Identify any implicit requirements or necessary prerequisites.\n",
    "\n",
    "2. **Break Down the Goal**:\n",
    "   - Decompose the goal into smaller, manageable sub-goals or tasks.\n",
    "   - Ensure each sub-goal is specific, actionable, and can be addressed with existing tools or data sources.\n",
    "   - Identify dependencies between sub-goals to establish the correct execution order.\n",
    "\n",
    "3. **Generate an Action Plan**:\n",
    "   - For each sub-goal, create a corresponding action step to achieve it.\n",
    "   - Ensure the plan follows the VM Specification.\n",
    "   - Include a 'reasoning' step at the beginning of the plan that outlines the chain of thought and dependency analysis of the steps.\n",
    "   - IMPORTANT: Always use tools within \"calling\" instructions. Never use tool functions directly in the plan.\n",
    "\n",
    "4. **Tool Usage Guidelines**:\n",
    "   - When using a tool, always wrap it in a \"calling\" instruction.\n",
    "   - For calling instruction, Only select tools listed in the \"Available Tools\" section. Using tools outside this list will cause the plan to fail.\n",
    "   - Ensure that the \"tool_params\" object contains all necessary parameters for the specific tool being called.\n",
    "\n",
    "The final step of the plan must be assign the final output result to the 'final_answer' variable.\n",
    "You should response in the following format:\n",
    "\n",
    "<think>...</think>\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"seq_no\": 0,\n",
    "    ...\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "where <think> is your detailed reasoning process in text format and the JSON array after <think> is a valid plan.\n",
    "\n",
    "### Response:\n",
    "<think>{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's tackle the goal: What is TiDB? I need to create a detailed action plan using the provided specifications. First, I should understand what TiDB is. TiDB is an open-source, real-time, in-memory, distributed database designed for handling complex queries and large datasets efficiently. It's built on top of TiDB's knowledge graph and vector search capabilities, making it highly scalable and suitable for applications requiring real-time analytics and machine learning.\n",
      "\n",
      "To achieve this, I'll break down the goal into manageable steps. The primary task is to retrieve information about TiDB using the available tools:retrieve_knowledge_graph and vector_search. Then, process this information to generate a coherent summary using llm_generate.\n",
      "\n",
      "1. **Retrieve Knowledge Graph Data**: Use retrieve_knowledge_graph to get structured information about TiDB. This will provide a comprehensive overview of TiDB's features, components, and use cases.\n",
      "\n",
      "2. **Retrieve Vector Search Snippets**: Fetch relevant documentation or examples from TiDB's vector search to get practical insights and code snippets.\n",
      "\n",
      "3. **Synthesize Information**: Combine the data from both sources to create a detailed summary. This involves highlighting TiDB's key features, performance capabilities, and use cases.\n",
      "\n",
      "4. **Generate Final Answer**: Use llm_generate to refine the synthesized information into a clear, concise, and well-structured response.\n",
      "\n",
      "I'll structure the plan using the specified JSON format, ensuring each step is a JSON object with the correct type and parameters. I'll make sure to use output_vars to store intermediate results for subsequent steps.\n",
      "\n",
      "Now, I'll draft the action plan following the guidelines, ensuring all tools are used within \"calling\" instructions and that the final answer is assigned to \"final_answer\".\n",
      "</think>\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"seq_no\": 0,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 1,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": \"42\",\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 2,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 3,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 4,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 5,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 6,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 7,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 8,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 9,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 10,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "```<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "goal = \"what is tidb?\"\n",
    "\n",
    "FastLanguageModel.for_inference(model) \n",
    "inputs = tokenizer([prompt_style.format(goal, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Your task is to generate a detailed action plan to achieve the following goal:\n",
    "\n",
    "Goal: {}\n",
    "\n",
    "--------------------------------\n",
    "\n",
    "**MUST follow the Specification**:\n",
    "# Specification for Generating Executable Plans for the Virtual Machine (VM)\n",
    "\n",
    "## Table of Contents\n",
    "1. Overview of the VM\n",
    "2. Instruction Format\n",
    "3. Supported Instructions\n",
    "4. Parameters and Variable References\n",
    "5. Variables and Dependencies\n",
    "6. Plan Structure\n",
    "7. Best Practices\n",
    "8. Common Errors\n",
    "9. Available Tools for calling instruction\n",
    "10. Example Plan\n",
    "\n",
    "## 1. Overview of the VM\n",
    "The VM executes plans consisting of a sequence of instructions. Each instruction performs a specific operation and may interact with variables stored in a variable store. The VM supports conditional execution and can handle dependencies between instructions through variable assignments and references.\n",
    "\n",
    "### Key features:\n",
    "- **Variable Store**: A key-value store where variables are stored and accessed by name.\n",
    "- **Instruction Execution**: Instructions are executed sequentially unless control flow is altered by conditional statements.\n",
    "\n",
    "## 2. Instruction Format\n",
    "Each instruction in the plan is represented as a JSON object with the following keys:\n",
    "\n",
    "- `seq_no`: A unique and AUTO-INCREMENT integer identifying the instruction's sequence within the plan, starting from 0.\n",
    "- `type`: A string indicating the instruction type. See Supported Instructions.\n",
    "- `parameters`: An object containing parameters required by the instruction.\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": N,\n",
    "  \"type\": \"instruction_type\",\n",
    "  \"parameters\": {{\n",
    "    \"param1\": \"value_or_variable_reference\",\n",
    "    \"param2\": \"value_or_variable_reference\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## 3. Supported Instructions\n",
    "### 3.1 assign\n",
    "- **Purpose**: Assigns values to one or more variables.\n",
    "- **Parameters**: An object where each key is a variable name. Each value can be:\n",
    "  1. A direct value (number/string).\n",
    "  2. A reference to an existing variable: use the syntax \"${{variable_name}}\".\n",
    "  3. A template string that interpolates variables for string concatenation.\n",
    "     - Example: \"The reason is: ${{reason}}, and the solution is: ${{solution}}\"\n",
    "  4. A basic arithmetic expression involving numeric variables:\n",
    "     - Supported operators: +, -, *, /, ** (pow), % (mod), unary +/-\n",
    "     - Example: \"${{var0}} / 3 + ${{var1}}\"\n",
    "\n",
    "The VM will:\n",
    "1. Replace each \"${{varName}}\" with the current value of varName.\n",
    "2. If the result is a pure numeric expression (e.g., 2+3, 5*6, or referencing numeric variables), it will be evaluated as a number.\n",
    "3. If the result is a string with placeholders, it becomes a string concatenation or template filling.\n",
    "4. Assign the final computed result back to the target variable(s).\n",
    "\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "1. Direct Assignment\n",
    "   ```json\n",
    "   {{\n",
    "     \"seq_no\": 0,\n",
    "     \"type\": \"assign\",\n",
    "     \"parameters\": {{\n",
    "       \"constant_number\": 42,\n",
    "       \"message\": \"Hello World\"\n",
    "     }}\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "2. Template/String Interpolation\n",
    "   ```json\n",
    "   {{\n",
    "     \"seq_no\": 1,\n",
    "     \"type\": \"assign\",\n",
    "     \"parameters\": {{\n",
    "       \"recommended_solution\": \"Reason: ${{reason}}\\nSolution: ${{solution}}\"\n",
    "     }}\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "3. Basic Arithmetic\n",
    "   ```json\n",
    "   {{\n",
    "     \"seq_no\": 2,\n",
    "     \"type\": \"assign\",\n",
    "     \"parameters\": {{\n",
    "       \"calculated_result\": \"${{num1}} + ${{num2}} / 3\"\n",
    "     }}\n",
    "   }}\n",
    "   ```\n",
    "\n",
    "### 3.2 jmp\n",
    "- **Purpose**: Jumps to a specified sequence number based on an optional condition.\n",
    "- **Parameters**:\n",
    "  - `condition_prompt` (optional): The prompt to evaluate the condition. If provided, the LLM evaluates whether to jump. **Must respond with a JSON object in the following format:**\n",
    "    ```json\n",
    "    {{\n",
    "      \"result\": boolean,\n",
    "      \"explanation\": string\n",
    "    }}\n",
    "    ```\n",
    "  - `context` (optional): Additional context for the LLM. Can be a direct string or a variable reference.\n",
    "  - `jump_if_true`: The `seq_no` to jump to if the condition evaluates to true. Required if `condition_prompt` is provided.\n",
    "  - `jump_if_false` (optional): The `seq_no` to jump to if the condition evaluates to false. Required if `condition_prompt` is provided.\n",
    "  - `target_seq` (optional): The `seq_no` to jump to if no condition is provided (unconditional jump).\n",
    "\n",
    "**Example (Conditional Jump):**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 4,\n",
    "  \"type\": \"jmp\",\n",
    "  \"parameters\": {{\n",
    "    \"condition_prompt\": \"Is ${{number}} even? Respond with a JSON object in the following format:\\n{{\\n  \\\"result\\\": boolean,\\n  \\\"explanation\\\": string\\n}}\\nWhere 'result' is true if the number is even, false otherwise, and 'explanation' provides a brief reason for the result.\",\n",
    "    \"context\": null,\n",
    "    \"jump_if_true\": 5,\n",
    "    \"jump_if_false\": 6\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**Example (Unconditional Jump):**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 5,\n",
    "  \"type\": \"jmp\",\n",
    "  \"parameters\": {{\n",
    "    \"target_seq\": 7\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "### 3.3 calling\n",
    "- **Purpose**: Invokes a specific tool or function with the provided parameters.\n",
    "- **Parameters**: Defines the specifications required to call a tool.\n",
    "  - `tool_name`: The name of the tool to be called for `calling` instruction.\n",
    "  - `tool_params`: An object containing key-value pairs that represent the arguments required by the specified tool.\n",
    "    - Keys: Must match the argument names expected by the tool.\n",
    "    - Values: Can be either a direct value or a variable reference.\n",
    "  - `output_vars` (optional): An array specifying how the tool's output should be stored in the VM's variable store for later use.\n",
    "    - If it is a string: The array contains one variable name. The entire tool's response is stored under this variable name.\n",
    "    - If it is an array: The array contains variable names corresponding to the keys in the JSON response. Each variable name in the array maps to a key in the JSON object, and the value associated with each key will be extracted and stored under the corresponding variable name.\n",
    "\n",
    "The structure of `calling` instruction:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": <unique_sequential_number>,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"<tool_name>\",\n",
    "    \"tool_params\": {{\n",
    "      <tool-specific parameters>\n",
    "    }},\n",
    "    \"output_vars\": [<list_of_output_variable_names>]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 1,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"tool_name\",\n",
    "    \"tool_params\": {{\n",
    "      \"param1\": \"value_or_variable_reference\",\n",
    "      \"param2\": \"value_or_variable_reference\"\n",
    "    }},\n",
    "    \"output_vars\": [\"variable_name_1\", ...]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## 4. Parameters and Variable References\n",
    "Parameters can be either direct values or variable references. To reference a variable, use the format `${{variable_name}}`.\n",
    "\n",
    "- **Direct Values** are used when you clearly know the corresponding parameter values. These values do not depend on the results of other instructions, ensuring clarity and simplicity. Using direct values helps improve query readability and maintainability, especially in scenarios where parameters do not need to change dynamically.\n",
    "\n",
    "- **Variable References** are ideal for scenarios that require dynamic parameter value filling, enhancing the interconnectivity and data flow between instructions. By using variable references, parameters can be adjusted dynamically based on the results of previous steps, increasing the flexibility and automation of the workflow.\n",
    "\n",
    "- **Don't Use Math Expressions in Parameters and tool_params**: The VM does not have the capability to compute or parse expressions within parameters. It can only perform simple reference substitutions. For example, avoid using expressions like value1 + value2 or value * 2 within parameters, and instead, calculate these values explicitly in a prior step and refer to the result in the parameter.\n",
    "\n",
    "\n",
    "**Direct Value Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 1,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"retrieve_knowledge_graph\",\n",
    "    \"tool_params\": {{\n",
    "      \"query\": \"TiDB latest stable version\"\n",
    "    }},\n",
    "    \"output_vars\": [\"latest_tidb_version_info\"]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "**Variable Reference Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 4,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"vector_search\",\n",
    "    \"tool_params\": {{\n",
    "      \"query\": \"What are the key features and improvements in TiDB version ${{latest_stable_tidb_version}}?\",\n",
    "      \"top_k\": 10\n",
    "    }},\n",
    "    \"output_vars\": [\"tidb_key_features_and_improvements\"]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "## 5. Variables and Dependencies\n",
    "- **Variable Assignment**: Use the `assign` instruction or specify an `output_vars` in a `calling` instruction that produces outputs.\n",
    "- **Variable Access**: Reference variables in parameters using the variable reference format.\n",
    "- **Dependencies**: Manage dependencies by assigning outputs to variables and referencing them in subsequent instructions.\n",
    "\n",
    "## 6. Plan Structure\n",
    "- **Sequential Execution**: Instructions are executed in order based on their `seq_no`.\n",
    "- **Control Flow**: Use the `jmp` instruction for branching logic and conditional loops.\n",
    "\n",
    "## 7. Best Practices\n",
    "- **Sequence Numbering**: Ensure that `seq_no` values are unique, sequential integers within the plan.\n",
    "- **Variable Naming**: Use descriptive variable names to make the plan readable and maintainable.\n",
    "- **Control Flow**: Use `jmp` instructions to create conditional logic, manage execution flow, and implement loops effectively.\n",
    "- **Final answer**: The name of output var of The last instruction MUST be \"final_answer\".\n",
    "- **Language Consistency**: All the instructions (e.g. `llm_generate`) that directly contribute to generating the `final_answer` must be written in the same language as the Response Language (if not specified, use the same language of the goal). This ensures the final output is consistent with the intended language.\n",
    "\n",
    "- **Instruction type selection**: Available instruction types:[assign, jmp, calling].\n",
    "\n",
    "- **Avoid variable dependencies within a single \"assign\" instruction**：Since the order of variable assignments within an \"assign\" instruction is not defined, do not rely on one variable being assigned before another within the same instruction. Instead, split assignments across multiple instructions if one depends on another. For example, this is incorrect:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 3,\n",
    "  \"type\": \"assign\",\n",
    "  \"parameters\": {{\n",
    "    \"y\": \"${{x}}\",\n",
    "    \"x\": 10\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "\"y\" might end up being undefined because we cannot guarantee that \"x\" will be set first. The correct approach is to split them:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 3,\n",
    "  \"type\": \"assign\",\n",
    "  \"parameters\": {{\n",
    "    \"x\": 10\n",
    "  }}\n",
    "}},\n",
    "{{\n",
    "  \"seq_no\": 4,\n",
    "  \"type\": \"assign\",\n",
    "  \"parameters\": {{\n",
    "    \"y\": \"${{x}}\"\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "- **Best Practices for Information Retrieval - Combining Knowledge Graph Search and Vector Search**:\n",
    "  - Dual Retrieval: When retrieving information, utilize both Knowledge Graph Search and Vector Search simultaneously. This combination enhances the richness of the information by leveraging the structured data from the knowledge graph and the detailed insights from vector search.\n",
    "  - Unified Summarization: After retrieving data from both tools, use an LLM generation tool to summarize the knowledge related to the query. Avoid directly using the loose data returned by the two tools; instead, ensure all retrieved information is processed through the LLM generation tool to create a coherent and well-structured final answer.\n",
    "  - Tool Integration: Ensure that raw data retrieved from both Knowledge Graph Search and Vector Search is exclusively processed by the LLM generation tool. Do not pass this data to other tools, as doing so may result in an unreadable final answer or prevent other tools from effectively processing the data. This practice maintains the coherence, integrity, and quality of the final response.\n",
    "  - Maintain Coherence: By processing all retrieved data through the LLM generation tool, you ensure that the final answer is a cohesive, single-language narrative. This avoids the inclusion of raw or fragmented data that could compromise the readability and consistency of the response.\n",
    "\n",
    "- **Final Answer Alignment**:\n",
    "  - **Goal-Centric Generation**: Ensure that the generated `final_answer` directly addresses the question or objective outlined in the goal. The `final_answer` should be focused and relevant to the goal and avoid general response.\n",
    "  - **Contextual Consistency**: Since the tools in the plan (e.g., `llm_generate`) do not aware the goal, include the goal context when making tool calls if necessary. Maintain the alignment between the goal and all intermediate steps leading to the `final_answer`. This ensures that every instruction and tool interaction contributes towards achieving the desired outcome.\n",
    "  - **Avoid Divergence**: Prevent the generation of information that, while relevant, does not serve to answer the primary goal. All synthesized and summarized data should reinforce the goal-centric `final_answer`.\n",
    "\n",
    "## 8. Common Errors\n",
    "\n",
    "**Case 1: Querying Specific Runtime/Environment Information**\n",
    "\n",
    "**Error Example:**\n",
    "```json\n",
    "{{\n",
    "  \"seq_no\": 1,\n",
    "  \"type\": \"calling\",\n",
    "  \"parameters\": {{\n",
    "    \"tool_name\": \"tool_name\",\n",
    "    \"tool_params\": {{\n",
    "      \"query\": \"Determin the current version of ...\"\n",
    "    }},\n",
    "    \"output_vars\": [...]\n",
    "  }}\n",
    "}}\n",
    "```\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"parameters\": {{\n",
    "    \"output_vars\": [\n",
    "      \"slow_query_log_explanation\",\n",
    "      \"sample_slow_query_log\"\n",
    "    ],\n",
    "    \"tool_name\": \"llm_generate\",\n",
    "    \"tool_params\": {{\n",
    "      \"context\": null,\n",
    "      \"prompt\": \"Please analyze the sql query: `SELECT * FROM INFORMATION_SCHEMA.SLOW_QUERY ORDER BY start_time DESC LIMIT 10;`. Explain the slow query and its relevant details(at least contain 'query', 'start_time', 'duration', 'plan_digest').\\n\\nPlease ensure that the generated text uses English.\"\n",
    "    }}\n",
    "  }},\n",
    "  \"seq_no\": 2,\n",
    "  \"type\": \"calling\"\n",
    "}}\n",
    "```\n",
    "\n",
    "**Error Explanation**:\n",
    "\n",
    "- **Not allowed to execute SQL**: Please do not use any tools, such as llm_generate, to attempt to obtain SQL execution results.\n",
    "- **Do Not Assume Specific Environment Information**: Do not make assumptions about (or generate) specific details of the environment, such as their current system configuration, current versions of tidb, current tiup version, or private data. Plans should be designed to be adaptable and not rely on presumed specific environment information.\n",
    "- **Avoid Obtain Specific Data with General Tools**: General tools like `retrieve_knowledge_graph`, `vector_search` and `llm_generate` can only access public documentation and general knowledge. They cannot access:\n",
    "  - Current system configuration\n",
    "  - Current version\n",
    "  - Cluster status\n",
    "  - Any private or runtime information\n",
    "  Such specific environment information can only be obtained through specialized tools explicitly designed for that purpose, or should be provided by the user as part of their query.\n",
    "\n",
    "\n",
    "## 9. Available Tools for `calling` instruction\n",
    "\n",
    "\n",
    "Please use only the following tools in Calling Instruction:\n",
    "\n",
    "### retrieve_knowledge_graph\n",
    "\n",
    "\n",
    "    Retrieves TiDB related information from a knowledge graph, returning nodes and relationships between those nodes.\n",
    "\n",
    "    This tool is designed to extract structured knowledge about TiDB from a knowledge graph. It excels at identifying entities and relationships, providing a rich context of interconnected information.\n",
    "\n",
    "    Arguments:\n",
    "      - `query`: The query string. This should be a question or statement about TiDB entities, concepts, or their relationships. Can be a direct string or a variable reference.\n",
    "\n",
    "    Output:\n",
    "      - Returns a single dictionary (`Dict`) representing the retrieved knowledge graph data. This dictionary contains a complex structure representing nodes and the relationships between them, extracted from the knowledge graph. \n",
    "        **Important:** The raw output of this tool, a complex dictionary representing graph data, is **not intended for direct use in the final answer.**  The knowledge graph data is returned in a structured format that requires further processing to be presented in a user-friendly and coherent manner.\n",
    "\n",
    "\n",
    "    Best practices:\n",
    "    - **Prioritize for Information Retrieval:** For most information retrieval tasks related to TiDB knowledge, the `retrieve_knowledge_graph` tool should be your **first choice**.  Consider using `retrieve_knowledge_graph` and `vector_search` together with the **same query** to retrieve complementary information and increase the richness of results.  `vector_search` can be used as a secondary option when graph-based knowledge is insufficient.\n",
    "    - **Refine and Synthesize with `llm_generate`:** After retrieving information using `retrieve_knowledge_graph` (and optionally `vector_search`), **always** process the raw output using the `llm_generate` tool.  Use `llm_generate` to refine, summarize, and synthesize the retrieved knowledge graph data (and document snippets if using `vector_search` as well) into a concise and user-friendly answer. Do **not** directly use the raw output in the `final_answer`.\n",
    "    - **Focus Queries on General TiDB Knowledge:**  Target your queries towards general, shared knowledge about TiDB concepts and relationships.  Avoid queries that are specific to a user's environment or seek private data like configurations or versions, which is out of scope of this tool.\n",
    "    \n",
    "\n",
    "### llm_generate\n",
    "\n",
    "\n",
    "    Generates a response using the Language Model (LLM).\n",
    "\n",
    "    This tool must be used within a \"calling\" instruction in the plan.\n",
    "\n",
    "    Arguments:\n",
    "    - `prompt`: The prompt to provide to the LLM. Can be a direct string or a variable reference.\n",
    "        - **Language Matching**: Write the prompt in the same language as the goal.\n",
    "        - **Language Confirmation**: Append a sentence to confirm the desired language of the generated text:\n",
    "            - *For English goals*: \"Please ensure that the generated text uses English.\"\n",
    "            - *For Chinese goals*: \"请确保生成的文本使用中文。\"\n",
    "            - *For Japanese goals*: \"Please ensure that the generated text uses Japanese.\"\n",
    "    - `context` (optional): Additional context for the LLM. Can be a direct string or a variable reference.\n",
    "\n",
    "    Output: The output format (text or JSON) depends on your instructions.\n",
    "    - Text Response: If you ask for a text answer, let output_vars be an array containing one variable name. The entire text response will be stored under this variable.\n",
    "    - JSON Response: If you instruct the LLM to respond in JSON format, let output_vars be an array containing variable names that match the keys in the JSON response. Each variable name corresponds to a key in the JSON object, and the value associated with each key is stored under the corresponding variable name.\n",
    "\n",
    "    Example usage in a plan:\n",
    "    ```json\n",
    "    {{\n",
    "        \"seq_no\": 1,\n",
    "        \"type\": \"calling\",\n",
    "        \"parameters\": {{\n",
    "            \"tool_name\": \"llm_generate\",\n",
    "            \"tool_params\": {{\n",
    "                \"prompt\": \"Analyze the sales data and provide summary and insights, response a json object including keys ['summary', 'insights'].\",\n",
    "                \"context\": \"${{sales_data}}\"\n",
    "            }},\n",
    "            \"output_vars\": [\"summary\", \"insights\"]\n",
    "        }}\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    Best practices:\n",
    "    - Always use llm_generate within a \"calling\" instruction in your plan.\n",
    "    - Use variable references (${{variable_name}}) when you need to include dynamic content from previous steps.\n",
    "    \n",
    "\n",
    "### vector_search\n",
    "\n",
    "\n",
    "    Retrieves the most relevant snippets of TiDB documentation based on embedding similarity to your query.\n",
    "\n",
    "    This tool leverages vector embeddings to find document fragments from TiDB documentation that are most semantically similar to your query. It excels at finding relevant document snippets that provide rich context and detailed information.\n",
    "\n",
    "    Arguments:\n",
    "      - `query`: The query string. It should be a clear and simple statement or question, focusing on a single objective for best results.\n",
    "      - `top_k`: The number of top document snippets to retrieve. Must be an integer or a variable referencing an integer.\n",
    "\n",
    "    Output:\n",
    "      - Returns a list of dictionaries (`List[Dict]`). Each dictionary represents a retrieved document chunk and contains information about the chunk (e.g., content, source). **Important:** The raw output of this tool, a list of dictionaries, is **not intended for direct use in the final answer.** The document chunks are returned as individual fragments and require further processing to form a coherent response.\n",
    "\n",
    "\n",
    "    Example to call this tool:\n",
    "\n",
    "    **Example:**\n",
    "    ```json\n",
    "    {{\n",
    "        \"seq_no\": 3,\n",
    "        \"type\": \"calling\",\n",
    "        \"parameters\": {{\n",
    "            \"tool_name\": \"vector_search\",\n",
    "            \"tool_params\": {{\n",
    "                \"query\": \"Information about ...\",\n",
    "                \"top_k\": 10\n",
    "            }},\n",
    "            \"output_vars\": [\"embedded_chunks\"]\n",
    "        }}\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    Best practices:\n",
    "      - **Process Output with `llm_generate`:**  The `vector_search` tool returns a list of document chunks. **Always** process this raw output using the `llm_generate` tool to summarize, synthesize, and refine the information into a coherent answer before using it in the final response.  Do **not** directly use the raw `vector_search` output in the `final_answer`.\n",
    "      - **Use Clear, Focused Queries:** For the best search results, ensure your query is clear, concise, and focuses on a **single**, specific question or objective. Avoid multi-part or ambiguous queries.\n",
    "\n",
    "-------------------------------\n",
    "\n",
    "Now, let's generate the plan.\n",
    "\n",
    "1. **Analyze the Request**:\n",
    "   - Determine the primary intent behind the goal.\n",
    "   - Identify any implicit requirements or necessary prerequisites.\n",
    "\n",
    "2. **Break Down the Goal**:\n",
    "   - Decompose the goal into smaller, manageable sub-goals or tasks.\n",
    "   - Ensure each sub-goal is specific, actionable, and can be addressed with existing tools or data sources.\n",
    "   - Identify dependencies between sub-goals to establish the correct execution order.\n",
    "\n",
    "3. **Generate an Action Plan**:\n",
    "   - For each sub-goal, create a corresponding action step to achieve it.\n",
    "   - Ensure the plan follows the VM Specification.\n",
    "   - Include a 'reasoning' step at the beginning of the plan that outlines the chain of thought and dependency analysis of the steps.\n",
    "   - IMPORTANT: Always use tools within \"calling\" instructions. Never use tool functions directly in the plan.\n",
    "\n",
    "4. **Tool Usage Guidelines**:\n",
    "   - When using a tool, always wrap it in a \"calling\" instruction.\n",
    "   - For calling instruction, Only select tools listed in the \"Available Tools\" section. Using tools outside this list will cause the plan to fail.\n",
    "   - Ensure that the \"tool_params\" object contains all necessary parameters for the specific tool being called.\n",
    "\n",
    "The final step of the plan must be assign the final output result to the 'final_answer' variable.\n",
    "You should response in the following format:\n",
    "\n",
    "<think>...</think>\n",
    "```json\n",
    "[\n",
    "  {{\n",
    "    \"seq_no\": 0,\n",
    "    ...\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "where <think> is your detailed reasoning process in text format and the JSON array after <think> is a valid plan.\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    goals = examples[\"goal\"]\n",
    "    metadatas = examples[\"metadata\"]\n",
    "    best_plans = examples[\"best_plan\"]\n",
    "    reasonings = examples[\"reasoning\"]\n",
    "    texts = []\n",
    "    for goal, metadata, cot, plan in zip(goals, metadatas, reasonings, best_plans):\n",
    "        metadata_json = json.loads(metadata)\n",
    "        goal_description = describe_goal(goal, metadata_json)\n",
    "        plan_json = json.loads(plan)\n",
    "        plan_description = f\"\"\"```json{plan_json}```\"\"\"\n",
    "        text = train_prompt_style.format(goal_description, cot, plan_description) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 100/100 [00:00<00:00, 11265.32 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['task_id', 'goal', 'metadata', 'best_plan', 'reasoning', 'final_answer'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ianthereal-z/tidb_bot\", split = \"train[0:]\",trust_remote_code=True)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=3407)  # 10% for validation\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "print(train_dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.1.8 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_training(model)\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=2): 100%|██████████| 100/100 [00:01<00:00, 55.30 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        num_train_epochs = 3,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,\n",
    "        learning_rate=1e-4,\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        logging_steps=5,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.1,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=5,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=5,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 100 | Num Epochs = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 16 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 40,370,176\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 25:01, Epoch 8/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.542300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.938500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.680800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.600500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.565300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "Alright, let's break down the goal of finding tidb's latest stable version. The primary intent is to obtain the latest stable version of TiDB, which is a specific runtime/environment information. Since tools like vector_search and llm_generate cannot access current system configurations or runtime details, we must obtain this information through a specialized tool, or it should be provided by the user.\n",
      "\n",
      "To achieve this, we can utilize the llm_generate tool within a \"calling\" instruction. The llm_generate tool can generate a response using the LLM, ensuring the answer is in the same language as the goal. Since the goal is in English, we'll use the English version of llm_generate.\n",
      "\n",
      "We need to include a \"reasoning\" step at the beginning of the plan to outline the chain of thought and dependency analysis. The reasoning should explain that we cannot obtain specific environment information (like current system configuration or private data) using the tools provided. We must rely on a specialized tool or provide this information explicitly.\n",
      "\n",
      "Finally, the final answer should be assigned to the \"final_answer\" variable.\n",
      "</think>\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"seq_no\": 0,\n",
      "    \"type\": \"reasoning\",\n",
      "    \"parameters\": {\n",
      "      \"output_vars\": [\"final_answer\"]\n",
      "    },\n",
      "    \"reasoning\": \"The goal is to find the latest stable version of TiDB. Since we cannot access current system configuration or private data, we rely on the LLM to generate a response in the same language as the goal. The LLM will summarize and synthesize relevant information about TiDB versions, ensuring the answer is presented in a user-friendly manner. We must provide this information explicitly as tools cannot retrieve it.\"\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 1,\n",
      "    \"type\": \"calling\",\n",
      "    \"parameters\": {\n",
      "        \"tool_name\": \"llm_generate\",\n",
      "        \"tool_params\": {\n",
      "            \"prompt\": \"Analyze the tidb version history and provide a summary and insights, response a JSON object including keys ['summary', 'insights'].\",\n",
      "            \"context\": \"Please analyze the tidb version history and provide a summary and insights. Response a JSON object including keys ['summary', 'insights'].\",\n",
      "            \"output_vars\": [\"summary\", \"insights\"]\n",
      "        }\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"seq_no\": 2,\n",
      "    \"type\": \"assign\",\n",
      "    \"parameters\": {\n",
      "      \"constant_number\": 42,\n",
      "      \"message\": \"Hello World\"\n",
      "    }\n",
      "  }\n",
      "]\n",
      "```<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "goal = \"what is tidb?\" \n",
    "\n",
    "FastLanguageModel.for_inference(model)  # Unsloth has 2x faster inference!\n",
    "inputs = tokenizer([prompt_style.format(goal, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    ")\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
